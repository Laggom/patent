"""Google Patents XHR ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ ìœ í‹¸ë¦¬í‹°

Playwrightë¡œ ì´ˆê¸° ì ‘ì†(ì„¸ì…˜/ì¿ í‚¤/í—¤ë”/ë³´ì•ˆ í† í° í™•ë³´) í›„,
í•´ë‹¹ ìƒíƒœë¥¼ httpxë¡œ ì¬ì‚¬ìš©í•˜ì—¬ XHR ê²€ìƒ‰ ë° íŠ¹í—ˆ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
  # PDF ë‹¤ìš´ë¡œë“œ
  python google_patents_xhr_downloader.py \
    --query "machine learning" \
    --out "./downloads" \
    --max-results 3 \
    --headless
    
  # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë§Œ í™•ì¸ (PDF ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°)
  python google_patents_xhr_downloader.py \
    --query "CO2 membrane inventor:\"Haiqing Lin\"" \
    --out "./downloads" \
    --count-only \
    --headless
    
  # ì—¬ëŸ¬ ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
  python google_patents_xhr_downloader.py \
    --query "quantum computing" \
    --query "machine learning" \
    --out "./downloads" \
    --count-only \
    --headless

ì°¸ê³ :
- XHR ì—”ë“œí¬ì¸íŠ¸(`/xhr/query`) ìš”ì²­ì„ ì‹¤ì œ ë¸Œë¼ìš°ì €ì—ì„œ í•œ ë²ˆ ë°œìƒì‹œì¼œ
  í•„ìš”í•œ í—¤ë”(`x-same-domain`, `user-agent`, `accept-language`, `cookie` ë“±)ì™€
  URL íŒŒë¼ë¯¸í„°(ë‚´ë¶€ í† í° í¬í•¨ ê°€ëŠ¥)ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤.
- ìº¡ì²˜ëœ ê°’ìœ¼ë¡œ httpx(HTTP/2) í´ë¼ì´ì–¸íŠ¸ë¥¼ êµ¬ì„±í•´ ë™ì¼í•œ ìš”ì²­ì„ ì¬í˜„í•©ë‹ˆë‹¤.
- ê²€ìƒ‰ ê²°ê³¼ì—ì„œ íŠ¹í—ˆ ìƒì„¸ë¡œ ì´ë™í•´ `meta[name="citation_pdf_url"]`ë¡œ PDF ì§ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

ì£¼ì˜: ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” êµ¬ê¸€ ì‚¬ì´íŠ¸ ë³€ê²½ì— ë¯¼ê°í•©ë‹ˆë‹¤. ì°¨ë‹¨(403/429) ì‹œ ì§„ë‹¨ ì•„í‹°íŒ©íŠ¸ë¥¼
      í™œì„±í™”(`--diagnostics`)í•´ ìº¡ì²˜ëœ ìš”ì²­ì„ í™•ì¸í•˜ê³  ì§€ì—°(`--delay`)ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”.

Google Patents ê²€ìƒ‰ ë¬¸ë²• ê°€ì´ë“œ (ìƒì„±í˜• AIìš© ì²´ê³„ì  ë§¤ë‰´ì–¼):

=== ê¸°ë³¸ êµ¬ë¬¸ êµ¬ì¡° ===

1. ì—°ì‚°ì ìš°ì„ ìˆœìœ„ (ë†’ìŒ â†’ ë‚®ìŒ):
   1ìˆœìœ„: ê´„í˜¸ ()
   2ìˆœìœ„: ê·¼ì ‘ì—°ì‚°ì (NEAR, SAME, WITH, ADJ)  
   3ìˆœìœ„: NOT (-)
   4ìˆœìœ„: AND (ê¸°ë³¸ê°’, ê³µë°±ë„ ANDë¡œ ì²˜ë¦¬)
   5ìˆœìœ„: OR

2. ê¸°ë³¸ ë…¼ë¦¬:
   - ê¸°ë³¸ ì—°ì‚°ì: AND (ê³µë°±ìœ¼ë¡œë„ í‘œí˜„)
   - ê²°í•©ì„±: ì¢Œê²°í•© (A OR B C â†’ (A OR B) AND C)
   - ê·¸ë£¹í•‘: ê´„í˜¸ () ì‚¬ìš© í•„ìˆ˜

=== ê²€ìƒ‰ ìš”ì†Œë³„ ë¬¸ë²• ===

A. í‚¤ì›Œë“œ ê²€ìƒ‰:
   ì •í™•êµ¬ë¬¸: "machine learning" (ë”°ì˜´í‘œ í•„ìˆ˜)
   ì œì™¸: -"deep learning" (NOT ì—°ì‚°)
   ì™€ì¼ë“œì¹´ë“œ: learn*, networ?, optim# (ë”°ì˜´í‘œ ì™¸ë¶€ì—ë§Œ)
   
B. í•„ë“œ ì œí•œ:
   ì œëª©: TI=(keyword)
   ì´ˆë¡: AB=(keyword)  
   ì²­êµ¬í•­: CL=(keyword)
   ì „ì²´í…ìŠ¤íŠ¸: keyword (í•„ë“œ ë¯¸ì§€ì •ì‹œ ì œëª©+ì´ˆë¡+ì²­êµ¬í•­+ë³¸ë¬¸ ê²€ìƒ‰)

C. ë¶„ë¥˜ ì½”ë“œ:
   ì •í™•ë§¤ì¹­: CPC=G06N3/08
   í•˜ìœ„í¬í•¨: CPC=G06N3
   
D. ë©”íƒ€ë°ì´í„°:
   ì¶œì›ì¸: assignee:"Company Name"
   ë°œëª…ì: inventor:"Last Name"
   ë‚ ì§œ: after:2020, before:2024
   êµ­ê°€: country:US, country:(US OR EP)
   ìƒíƒœ: status:grant, status:application

E. ê·¼ì ‘ ì—°ì‚°ì (ë­í‚¹ìš©, í•„í„°ë§ ì•„ë‹˜):
   ê±°ë¦¬ì§€ì •: NEAR/5 (5ë‹¨ì–´ ì´ë‚´, ìˆœì„œë¬´ê´€)
   ì¸ì ‘: ADJ/2 (2ë‹¨ì–´ ì´ë‚´, ìˆœì„œìœ ì§€)
   ë¬¸ë‹¨ë‚´: SAME (200ë‹¨ì–´ ì´ë‚´)
   ë¬¸ì¥ë‚´: WITH (20ë‹¨ì–´ ì´ë‚´)

=== ì˜¬ë°”ë¥¸ êµ¬ë¬¸ íŒ¨í„´ ===

âœ… ë‹¨ìˆœ ì¡°í•©:
   machine learning AND neural network
   TI=(artificial intelligence) AND assignee:"Google"
   
âœ… ê·¼ì ‘ ì—°ì‚°ì:
   machine NEAR/3 learning SAME neural
   TI=(quantum ADJ/1 computing)
   
âœ… ë³µì¡í•œ ê·¸ë£¹í•‘:
   (TI=(quantum) OR AB=(quantum)) AND CL=(computing)
   ((A NEAR/3 B) AND (C ADJ D)) OR (E WITH F)

âœ… ë©”íƒ€ë°ì´í„° ì¡°í•©:
   "machine learning" AND assignee:"Google" AND after:2020 AND CPC=G06N

=== ì ˆëŒ€ ê¸ˆì§€ íŒ¨í„´ ===

âŒ ì—°ì‚°ì í˜¼ìš©:
   word1 AND NEAR/5 word2    // AND ë’¤ì— ê·¼ì ‘ì—°ì‚°ì
   word1 OR SAME word2       // OR ë’¤ì— ê·¼ì ‘ì—°ì‚°ì
   
âŒ ê´„í˜¸ ì˜¤ë¥˜:
   ((((word1                 // ë¯¸ë§¤ì¹­ ê´„í˜¸
   TI=(word1] AND [AB=word2   // ëŒ€ê´„í˜¸ í˜¼ìš©
   
âŒ ì¸ìš©ë¶€í˜¸ ë‚´ íŠ¹ìˆ˜ë¬¸ì:
   "machine learn*"          // ì¸ìš©ë¶€í˜¸ ì•ˆì— ì™€ì¼ë“œì¹´ë“œ
   "word1 NEAR/3 word2"      // ì¸ìš©ë¶€í˜¸ ì•ˆì— ì—°ì‚°ì

=== ê²€ìƒ‰ì‹ ì‘ì„± ì•Œê³ ë¦¬ì¦˜ ===

1. í•µì‹¬ í‚¤ì›Œë“œ ì‹ë³„ â†’ ì •í™•êµ¬ë¬¸ì€ ë”°ì˜´í‘œ, ë³€í˜•ì–´ëŠ” ì™€ì¼ë“œì¹´ë“œ
2. ì¤‘ìš”ë„ë³„ í•„ë“œ ì„ íƒ â†’ ì œëª© > ì´ˆë¡ > ì²­êµ¬í•­ ìˆœ
3. ë©”íƒ€ë°ì´í„° ì¡°ê±´ ì¶”ê°€ â†’ ì¶œì›ì¸, ë‚ ì§œ, ë¶„ë¥˜ì½”ë“œ
4. ê·¼ì ‘ë„ ì§€ì • â†’ ê´€ë ¨ì„± ë†’ì€ ë‹¨ì–´ê°„ NEAR/ADJ ì ìš©  
5. ë…¼ë¦¬ êµ¬ì¡° ì„¤ê³„ â†’ ê´„í˜¸ë¡œ ëª…í™•í•œ ê·¸ë£¹í•‘
6. êµ¬ë¬¸ ê²€ì¦ â†’ ê´„í˜¸ ë§¤ì¹­, ì—°ì‚°ì ì¡°í•© í™•ì¸

=== ì‹¤ìš©ì  í•œê³„ (í…ŒìŠ¤íŠ¸ ê²€ì¦ë¨) ===

ì œí•œ ì—†ìŒ: AND ì¡°ê±´ ê°œìˆ˜ (13ê°œ+ ì„±ê³µ)
ì œí•œ ì—†ìŒ: ê·¼ì ‘ì—°ì‚°ì ì¤‘ì²© (6ì¤‘+ ì„±ê³µ)  
ì œí•œ ì—†ìŒ: í•„ë“œ ì¡°í•© (TI+AB+CL+ë©”íƒ€ë°ì´í„°)
ì œí•œ ì—†ìŒ: ë³µì¡í•œ ì¤‘ì²© ê´„í˜¸ êµ¬ì¡°

â˜… ê²€ìƒ‰ ì‹¤íŒ¨ ê·œì¹™ (2025ë…„ ì²´ê³„ì  í…ŒìŠ¤íŠ¸ ê²°ê³¼):

ê²€ìƒ‰ ì‹¤íŒ¨ ìœ í˜•:
1. êµ¬ë¬¸ ì˜¤ë¥˜ â†’ {"results":{"user_error":"invalid argument: query syntax error"}}
2. 0ê±´ ê²°ê³¼ â†’ {"results":{"total_num_results":0, ...}} (ì •ìƒ ì‘ë‹µ)

âŒ ì ˆëŒ€ ì•ˆë˜ëŠ” êµ¬ë¬¸ ì˜¤ë¥˜ íŒ¨í„´:
- ì˜ëª»ëœ ì—°ì‚°ì ì¡°í•©: "word1 AND NEAR/5 word2", "word1 OR SAME word2"
- ê´„í˜¸ ë§¤ì¹­ ì˜¤ë¥˜: "((((word1", "word1))))"
- ëŒ€ê´„í˜¸ í˜¼ìš©: "TI=(word1] AND [AB=word2"
- ì¸ìš©ë¶€í˜¸ ë‚´ íŠ¹ìˆ˜ë¬¸ì: "training data*", "word1 NEAR/3 word2", "machine learn*"

âœ… ì˜¬ë°”ë¥¸ êµ¬ë¬¸ íŒ¨í„´:
- ê·¼ì ‘ì—°ì‚°ì ì—°ì†: "word1 NEAR/5 word2 SAME word3"
- ê´„í˜¸ ê·¸ë£¹í•‘: "(word1 NEAR/3 word2) AND (word3 ADJ word4)"
- ì •í™•í•œ ì¸ìš©ë¶€í˜¸: "exact phrase", pretraining OR "training data"
- ì™€ì¼ë“œì¹´ë“œ: word* AND learn* (ì¸ìš©ë¶€í˜¸ ì™¸ë¶€ì—ì„œë§Œ)

ì‹¤ì œ í•œê³„ (í…ŒìŠ¤íŠ¸ ì™„ë£Œ):
- AND ì¡°ê±´ ê°œìˆ˜: 13ê°œ+ ì„±ê³µ (ë¬´ì œí•œ)
- ê·¼ì ‘ì—°ì‚°ì ì¤‘ì²©: 6ì¤‘+ ì„±ê³µ (NEAR+SAME+WITH+ADJ+NEAR+NEAR)
- í•„ë“œ ì¡°í•©: TI+AB+CL+ë©”íƒ€ë°ì´í„° ì„±ê³µ (ë¬´ì œí•œ)
- ë³µì¡í•œ ì¤‘ì²© ê´„í˜¸: ((A AND B) OR (C WITH D)) AND E ì„±ê³µ
- ë³µì¡ë„ëŠ” ë¬¸ì œì—†ìŒ, ì˜¤ì§ êµ¬ë¬¸ ì˜¤ë¥˜ë§Œì´ ì‹¤íŒ¨ ì›ì¸

ì˜¤ë¥˜ vs 0ê±´ êµ¬ë¶„ë²•:
if "user_error" in response: 
    # êµ¬ë¬¸ ì˜¤ë¥˜ â†’ ì¿¼ë¦¬ ë¬¸ë²• ìˆ˜ì • í•„ìš”
elif response["results"]["total_num_results"] == 0:
    # 0ê±´ ê²°ê³¼ â†’ ì¡°ê±´ ì™„í™” ê³ ë ¤ (ì •ìƒ ë™ì‘)

ê²°ë¡ : Google PatentsëŠ” ë³µì¡ë„ì— ë§¤ìš° ê´€ëŒ€í•˜ì§€ë§Œ êµ¬ë¬¸ ì˜¤ë¥˜ì— ë§¤ìš° ì—„ê²©í•¨

"""

from __future__ import annotations

import asyncio
import json
import re
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import httpx
from bs4 import BeautifulSoup
from loguru import logger
from playwright.async_api import Browser, BrowserContext, Page, async_playwright


GOOGLE_PATENTS_ORIGIN = "https://patents.google.com"


@dataclass
class CapturedRequest:
    """ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì œ ë°œìƒí•œ XHR ìš”ì²­ ì •ë³´.

    Attributes:
        url: í˜¸ì¶œëœ ì „ì²´ URL (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° í¬í•¨)
        method: HTTP ë©”ì„œë“œ
        headers: ìš”ì²­ í—¤ë”(ì¤‘ë³µ í‚¤ëŠ” ë§ˆì§€ë§‰ ê°’ ìœ ì§€)
        referer: ì°¸ì¡° í˜ì´ì§€ URL
    """

    url: str
    method: str
    headers: Dict[str, str]
    referer: Optional[str]


@dataclass
class PatentSummary:
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¶”ì¶œëœ íŠ¹í—ˆ ìš”ì•½ ì •ë³´."""

    title: str
    publication_number: Optional[str]
    detail_url: str


class GooglePatentsXHRDownloader:
    """Google Patentsì—ì„œ XHRë¡œ ê²€ìƒ‰í•˜ê³  PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë‹¤ìš´ë¡œë”."""

    def __init__(
        self,
        download_dir: Path,
        headless: bool = True,
        timeout: int = 30,
        delay: float = 1.0,
        diagnostics: bool = False,
    ) -> None:
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.headless = headless
        self.timeout = timeout
        self.delay = delay
        self.diagnostics = diagnostics
        self.diagnostics_dir = self.download_dir / "diagnostics"
        if self.diagnostics:
            self.diagnostics_dir.mkdir(parents=True, exist_ok=True)

    async def _launch_browser(self) -> Tuple[Browser, BrowserContext, Page]:
        """ë¸Œë¼ìš°ì €/ì»¨í…ìŠ¤íŠ¸/í˜ì´ì§€ë¥¼ ì´ˆê¸°í™”í•œë‹¤."""

        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(headless=self.headless)
        context = await browser.new_context()
        page = await context.new_page()
        page.set_default_timeout(self.timeout * 1000)
        return browser, context, page

    @staticmethod
    def _normalize_query_string(query: str) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ Google Patentsì˜ í‘œì¤€ êµ¬ë¬¸ìœ¼ë¡œ ì •ê·œí™”í•œë‹¤.

        Google PatentsëŠ” ëª…ì‹œì  í•„ë“œ ì ‘ë‘ì‚¬(abstract:/title:/claims:)ë³´ë‹¤
        ì•½ì–´ êµ¬ë¬¸(AB=/TI=/CL=)ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        DE=(Description) í•„ë“œëŠ” ì—°ì‚°ì ë™ì‘ì´ ë¶ˆì•ˆì •í•˜ì—¬ ë³€í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

        ì •ê·œí™” ê·œì¹™:
        1. í•„ë“œ ì•½ì–´ ë³€í™˜ (ëŒ€ì†Œë¬¸ì ë¬´ê´€):
           - abstract: â†’ AB=, title: â†’ TI=, claims: â†’ CL=
        2. ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ê·œí™”:
           - assignee: â†’ assignee:, inventor: â†’ inventor:
           - before:/after: â†’ before:/after:, country: â†’ country:
           - status: â†’ status:, language: â†’ language:

        ì´ë¯¸ í‘œì¤€ êµ¬ë¬¸ì„ ì‚¬ìš© ì¤‘ì´ë©´ ë³€í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """

        # í•„ë“œ ì•½ì–´ ì¹˜í™˜
        field_replacements = {
            r"\babstract:\s*": "AB=",
            r"\btitle:\s*": "TI=", 
            r"\bclaims:\s*": "CL=",
        }
        
        # ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ê·œí™” (ì¼ê´€ì„±ì„ ìœ„í•´)
        metadata_replacements = {
            r"\bassignee\s*=\s*": "assignee:",
            r"\binventor\s*=\s*": "inventor:",
            r"\bcountry\s*=\s*": "country:",
            r"\bstatus\s*=\s*": "status:",
            r"\blanguage\s*=\s*": "language:",
        }

        normalized = query
        
        # í•„ë“œ ì•½ì–´ ì¹˜í™˜ ì ìš©
        for pattern, repl in field_replacements.items():
            normalized = re.sub(pattern, repl, normalized, flags=re.IGNORECASE)
            
        # ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ê·œí™” ì ìš©
        for pattern, repl in metadata_replacements.items():
            normalized = re.sub(pattern, repl, normalized, flags=re.IGNORECASE)
            
        return normalized

    async def _try_accept_consent(self, page: Page) -> None:
        """êµ¬ê¸€ ë™ì˜(Consent) ë°°ë„ˆê°€ ìˆëŠ” ê²½ìš° ìµœëŒ€í•œ ë‹«ëŠ”ë‹¤."""

        async def try_click_targets(target_page: Page) -> bool:
            selectors = [
                "button:has-text('I agree')",
                "button:has-text('Agree')",
                "button:has-text('Accept all')",
                "button[aria-label*='Agree']",
                "#L2AGLb",
                "#introAgreeButton",
                "form[action*='consent'] button[type='submit']",
                "[role='dialog'] button:has-text('Accept')",
                "[role='dialog'] button:has-text('ë™ì˜')",
            ]
            for sel in selectors:
                try:
                    count = await target_page.locator(sel).count()
                    if count:
                        await target_page.locator(sel).first.click()
                        await target_page.wait_for_timeout(500)
                        return True
                except Exception:
                    continue
            return False

        try:
            # 1) í˜„ì¬ í˜ì´ì§€ì—ì„œ ì‹œë„
            if await try_click_targets(page):
                return

            # 2) consent iframe ë‚´ë¶€ì—ì„œ ì‹œë„
            for frame in page.frames:
                try:
                    url = (frame.url or "").lower()
                except Exception:
                    url = ""
                if "consent" in url or "privacy" in url:
                    try:
                        if await try_click_targets(frame):
                            return
                    except Exception:
                        continue
        except Exception:
            pass

    async def _capture_xhr_request(
        self, page: Page, query: str, diag_dir: Optional[Path] = None
    ) -> Tuple[Optional[CapturedRequest], str, Optional[str]]:
        """ê²€ìƒ‰ ì¤‘ `/xhr/query` ìš”ì²­ì„ í•˜ë‚˜ ìº¡ì²˜í•œë‹¤.

        ìš°ì„  ê¸°ë³¸ í™ˆì—ì„œ ì…ë ¥â†’ì—”í„°ë¡œ ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ ê²€ìƒ‰ URLë¡œ ì§ì ‘ ì´ë™í•´
        ê²°ê³¼ HTMLì„ í™•ë³´í•œë‹¤.

        Returns:
            (captured, search_results_html, xhr_response_text)
        """

        captured: Optional[CapturedRequest] = None
        search_results_html: str = ""
        xhr_response_text: Optional[str] = None

        # í•„ë“œ ë³„ì¹­ ì •ê·œí™”(abstract:/title:/claims: â†’ AB=/TI=/CL=)
        effective_query = self._normalize_query_string(query)

        def on_request(request: Any) -> None:
            nonlocal captured
            try:
                url = request.url
                if "/xhr/query" in url:
                    headers: Dict[str, str] = {}
                    for k, v in request.headers.items():
                        headers[k.lower()] = v
                    captured = CapturedRequest(
                        url=url,
                        method=request.method,
                        headers=headers,
                        referer=request.headers.get("referer"),
                    )
            except Exception:
                pass

        page.on("request", on_request)

        # 1ì°¨: í™ˆìœ¼ë¡œ ì§„ì…í•´ ì…ë ¥â†’ì—”í„°
        try:
            await page.goto(GOOGLE_PATENTS_ORIGIN)
            await self._try_accept_consent(page)
            await page.wait_for_load_state("domcontentloaded")

            # ìš°ì„  ê²€ìƒ‰ input(id=searchInput) ì‹œë„, ì—†ìœ¼ë©´ ì¼ë°˜ input ì‚¬ìš©
            try:
                await page.wait_for_selector("#searchInput", timeout=3000)
                await page.locator("#searchInput").fill(effective_query)
            except Exception:
                await page.wait_for_selector("input")
                await page.locator("input").first.fill(effective_query)
            await page.keyboard.press("Enter")

            try:
                resp = await page.wait_for_response(
                    lambda r: "/xhr/query" in r.url, timeout=self.timeout * 1000
                )
                try:
                    xhr_response_text = await resp.text()
                except Exception:
                    xhr_response_text = None
            except Exception:
                xhr_response_text = None

            # ê²°ê³¼ê°€ ì˜¬ë¼ì˜¬ ë•Œê¹Œì§€ ë³´ì¡° ëŒ€ê¸°(ê²°ê³¼ ì»¨í…Œì´ë„ˆ ë˜ëŠ” article)
            try:
                await page.wait_for_selector(
                    "article, state-modifier.result-title, #resultsContainer",
                    timeout=min(15000, self.timeout * 1000),
                )
            except Exception:
                pass

            try:
                await page.wait_for_load_state("networkidle", timeout=1500)
            except Exception:
                pass
            await page.wait_for_timeout(250)

            try:
                search_results_html = await page.content()
            except Exception:
                search_results_html = ""
        except Exception:
            search_results_html = ""

        # 2ì°¨ í´ë°±: ê²€ìƒ‰ URL ì§ì ‘ ì´ë™
        if not search_results_html or "<article" not in search_results_html:
            from urllib.parse import quote_plus
            search_url = f"{GOOGLE_PATENTS_ORIGIN}/?q={quote_plus(effective_query)}&hl=en&num=100"
            try:
                await page.goto(search_url)
                await self._try_accept_consent(page)
                try:
                    await page.wait_for_selector(
                        "article, state-modifier.result-title, #resultsContainer",
                        timeout=min(15000, self.timeout * 1000),
                    )
                except Exception:
                    pass
                try:
                    await page.wait_for_load_state("networkidle", timeout=1500)
                except Exception:
                    pass
                await page.wait_for_timeout(250)
                search_results_html = await page.content()
            except Exception:
                # ë§ˆì§€ë§‰ ì‹œë„: domcontentloaded ê¸°ì¤€ìœ¼ë¡œë¼ë„ HTML í™•ë³´
                try:
                    await page.wait_for_load_state("domcontentloaded")
                    search_results_html = await page.content()
                except Exception:
                    pass

        # ì§„ë‹¨ íŒŒì¼ ì €ì¥
        if self.diagnostics:
            try:
                target_dir = diag_dir or self.diagnostics_dir
                target_dir.mkdir(parents=True, exist_ok=True)
                (target_dir / "search_results_page.html").write_text(
                    search_results_html or "", encoding="utf-8"
                )
                if xhr_response_text is not None:
                    (target_dir / "xhr_query_response_original.html").write_text(
                        xhr_response_text, encoding="utf-8"
                    )
            except Exception:
                pass

        return captured, search_results_html, xhr_response_text

    # ë¶ˆìš© í•¨ìˆ˜ ì œê±°: _build_client_from_contextëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‚­ì œ

    @staticmethod
    async def _build_client_with_cookies(
        context: BrowserContext, captured: Optional[CapturedRequest]
    ) -> httpx.AsyncClient:
        """Playwright ì»¨í…ìŠ¤íŠ¸ì˜ ì¿ í‚¤/í—¤ë”ë¡œ httpx.AsyncClient ìƒì„±."""

        cookies_list = await context.cookies()
        cookies_jar = httpx.Cookies()
        for c in cookies_list:
            # httpx ì¿ í‚¤ì— ë„ë©”ì¸/ê²½ë¡œ ì§€ì •
            domain = c.get("domain") or ".google.com"
            path = c.get("path") or "/"
            cookies_jar.set(
                name=c.get("name"),
                value=c.get("value"),
                domain=domain,
                path=path,
            )

        default_headers: Dict[str, str] = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/126.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9,ko;q=0.8",
            "Accept": "text/html,application/xhtml+xml,application/xml;"
            "q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Referer": GOOGLE_PATENTS_ORIGIN + "/",
        }

        if captured:
            # ì¤‘ìš” í—¤ë”ë§Œ ì„ ë³„ ë°˜ì˜
            if ua := captured.headers.get("user-agent"):
                default_headers["User-Agent"] = ua
            if al := captured.headers.get("accept-language"):
                default_headers["Accept-Language"] = al
            if ref := captured.referer:
                default_headers["Referer"] = ref
            # x-same-domainì€ ì¢…ì¢… ìš”êµ¬ë¨
            if xsd := captured.headers.get("x-same-domain"):
                default_headers["x-same-domain"] = xsd
            else:
                default_headers["x-same-domain"] = "1"

        client = httpx.AsyncClient(
            headers=default_headers,
            cookies=cookies_jar,
            http2=True,
            timeout=httpx.Timeout(30.0),
        )
        return client

    @staticmethod
    def _parse_results_from_html(html: str) -> List[PatentSummary]:
        """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€(ì „ì²´ HTML)ì—ì„œ íŠ¹í—ˆ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±."""

        if not html:
            return []

        soup = BeautifulSoup(html, "lxml")
        results: List[PatentSummary] = []

        for article in soup.select("article"):
            try:
                title_el = article.select_one("h3 a")
                if not title_el:
                    continue
                title = (title_el.get_text() or "").strip()
                href = title_el.get("href") or ""
                if href and not href.startswith("http"):
                    detail_url = GOOGLE_PATENTS_ORIGIN + href
                else:
                    detail_url = href

                pub = None
                h4_el = article.select_one("h4")
                if h4_el:
                    # ì¢…ì¢… h4 ë‚´ë¶€ aì˜ í…ìŠ¤íŠ¸ê°€ ê³µë³´ë²ˆí˜¸
                    pub_link = h4_el.select_one("a")
                    if pub_link and pub_link.get_text():
                        pub = pub_link.get_text().strip()

                if title and detail_url:
                    results.append(
                        PatentSummary(
                            title=title, publication_number=pub, detail_url=detail_url
                        ))
            except Exception:
                continue

        return results

    async def _parse_results_from_dom(self, page: Page) -> List[PatentSummary]:
        """Playwright DOM APIë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§ì ‘ íŒŒì‹±í•œë‹¤.

        XHR ì‘ë‹µ ë˜ëŠ” ì •ì  HTML íŒŒì‹±ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ì˜ ë§ˆì§€ë§‰ í´ë°±.
        """
        results: List[PatentSummary] = []
        try:
            # ê²°ê³¼ê°€ ëŠë¦¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš° ëŒ€ë¹„
            await page.wait_for_selector("article", timeout=self.timeout * 1000)
        except Exception:
            # articleì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            return results

        try:
            articles = await page.query_selector_all("article")
        except Exception:
            articles = []

        for article in articles:
            try:
                title_el = await article.query_selector("h3 a")
                if not title_el:
                    continue
                raw_title = (await title_el.inner_text()) or ""
                title = raw_title.strip()
                href = await title_el.get_attribute("href")
                if not href:
                    continue
                if not href.startswith("http"):
                    detail_url = GOOGLE_PATENTS_ORIGIN + href
                else:
                    detail_url = href

                pub = None
                h4_link = await article.query_selector("h4 a")
                if h4_link:
                    try:
                        pub_text = await h4_link.inner_text()
                        if pub_text:
                            pub = pub_text.strip()
                    except Exception:
                        pub = None

                results.append(
                    PatentSummary(
                        title=title or (pub or ""),
                        publication_number=pub,
                        detail_url=detail_url,
                    ))
            except Exception:
                continue

        return results

    @staticmethod
    def _parse_results_from_xhr(content: str) -> Tuple[List[PatentSummary], Optional[int]]:
        """/xhr/query ì‘ë‹µ íŒŒì‹±.

        - JSON ë³¸ë¬¸: cluster â†’ result[] â†’ id, patent.publication_number, patent.title ì‚¬ìš©
        - HTML fragment ë³¸ë¬¸: ê¸°ì¡´ HTML íŒŒì„œ ì¬ì‚¬ìš©
        
        Returns:
            Tuple[List[PatentSummary], Optional[int]]: (íŠ¹í—ˆ ëª©ë¡, ì´ ê²°ê³¼ ê°œìˆ˜)
        """

        if not content:
            return [], None

        # 1) JSON ì‘ë‹µ ì‹œ
        try:
            if content.strip().startswith("{"):
                data = json.loads(content)
                results_node = data.get("results") or {}
                clusters = results_node.get("cluster") or []
                total_results = results_node.get("total_num_results")
                results: List[PatentSummary] = []
                for cluster in clusters:
                    for item in cluster.get("result", []):
                        item_id = item.get("id")  # ì˜ˆ: "patent/US11056471B2/en"
                        pat = item.get("patent") or {}
                        pub = pat.get("publication_number")
                        raw_title = pat.get("title") or ""
                        # ì œëª©ì— í¬í•¨ëœ íƒœê·¸ ì œê±°
                        title = BeautifulSoup(raw_title, "lxml").get_text(" ").strip()

                        if not item_id:
                            continue
                        detail_url = GOOGLE_PATENTS_ORIGIN + "/" + item_id.lstrip("/")

                        results.append(
                            PatentSummary(
                                title=title or (pub or ""),
                                publication_number=pub,
                                detail_url=detail_url,
                            ))
                return results, total_results
        except Exception:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ HTML ë¡œì§ìœ¼ë¡œ í´ë°±
            pass

        # 2) HTML fragment í´ë°± (ì´ ê²°ê³¼ ê°œìˆ˜ëŠ” HTMLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ None)
        html_results = GooglePatentsXHRDownloader._parse_results_from_html(content)
        return html_results, None

    @staticmethod
    def _sanitize_filename(filename: str) -> str:
        clean = re.sub(r"[<>:\"/\\|?*]", "_", filename)
        return clean[:200]

    async def _fetch_detail_and_pdf(
        self, client: httpx.AsyncClient, detail_url: str
    ) -> Optional[str]:
        """íŠ¹í—ˆ ìƒì„¸ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ PDF URLì„ ì¶”ì¶œí•œë‹¤."""

        r = await client.get(detail_url)
        if r.status_code >= 400:
            logger.warning(f"detail GET {detail_url} -> {r.status_code}")
            return None

        html = r.text
        soup = BeautifulSoup(html, "lxml")

        # 1ìˆœìœ„: citation_pdf_url
        meta = soup.select_one('meta[name="citation_pdf_url"]')
        if meta and meta.get("content"):
            return str(meta.get("content"))

        # 2ìˆœìœ„: a[href*='.pdf'] ë§í¬
        a = soup.select_one("a[href$='.pdf'], a[href*='.pdf?']")
        if a and a.get("href"):
            href = a.get("href")
            if href.startswith("http"):
                return href
            return GOOGLE_PATENTS_ORIGIN + href

        return None

    async def _download_pdf(
        self, client: httpx.AsyncClient, pdf_url: str, target_path: Path, referer: str
    ) -> bool:
        """PDFë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì €ì¥í•œë‹¤."""

        headers = {"Referer": referer, "Accept": "application/pdf,*/*"}
        try:
            async with client.stream("GET", pdf_url, headers=headers) as resp:
                if resp.status_code >= 400:
                    logger.error(
                        f"PDF GET {pdf_url} -> {resp.status_code}"
                    )
                    return False
                with target_path.open("wb") as f:
                    async for chunk in resp.aiter_bytes():
                        f.write(chunk)
            return True
        except Exception as exc:
            logger.error(f"PDF download error: {exc}")
            return False
    
    async def _fetch_all_results(
        self, 
        client: httpx.AsyncClient, 
        captured: CapturedRequest, 
        total_count: int,
        replay_headers: Dict[str, str],
        target_patent: Optional[str] = None
    ) -> List[PatentSummary]:
        """ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í˜ì´ì§€ë³„ë¡œ ìˆ˜ì§‘ (Seed Recall ê³„ì‚°ìš©)"""
        all_results = []
        page_size = 100  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        # URL íŒŒë¼ë¯¸í„° íŒŒì‹±
        from urllib.parse import urlparse, parse_qs
        parsed_url = urlparse(captured.url)
        base_params = parse_qs(parsed_url.query)
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ dictë¡œ ë³€í™˜
        params = {}
        for k, v in base_params.items():
            params[k] = v[0] if v else ""
        
        pages_fetched = 0
        max_pages = (total_count + page_size - 1) // page_size  # ì˜¬ë¦¼ ê³„ì‚°
        
        logger.info(f"Fetching up to {max_pages} pages ({total_count} total results)")
        
        for page in range(max_pages):
            start_idx = page * page_size
            params["num"] = str(min(page_size, total_count - start_idx))
            params["start"] = str(start_idx)
            
            # URL ì¬êµ¬ì„±
            query_string = "&".join([f"{k}={v}" for k, v in params.items() if v])
            fetch_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{query_string}"
            
            try:
                logger.info(f"Fetching page {page + 1}/{max_pages} (results {start_idx + 1}-{start_idx + int(params['num'])})")
                
                resp = await client.get(fetch_url, headers=replay_headers, timeout=15.0)
                if resp.status_code >= 400:
                    logger.warning(f"Page {page + 1} failed with status {resp.status_code}")
                    break
                
                page_results, _ = self._parse_results_from_xhr(resp.text)
                all_results.extend(page_results)
                pages_fetched += 1
                
                # Early termination: íƒ€ê²Ÿ íŠ¹í—ˆë¥¼ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                if target_patent:
                    normalized_target = target_patent.upper().replace(" ", "").replace(",", "").replace("-", "")
                    for patent in page_results:
                        if patent.publication_number:
                            normalized_found = patent.publication_number.upper().replace(" ", "").replace(",", "").replace("-", "")
                            if normalized_found == normalized_target:
                                logger.info(f"ğŸ¯ Target patent {target_patent} found on page {page + 1}! Early termination.")
                                return all_results
                
                # ì§„í–‰ë¥  ë¡œê·¸
                if pages_fetched % 5 == 0 or page == max_pages - 1:
                    logger.info(f"Progress: {len(all_results)}/{total_count} results fetched ({len(all_results)/total_count*100:.1f}%)")
                
                # ìš”ì²­ ê°„ ì§€ì—°
                await asyncio.sleep(0.5)
                
            except Exception as exc:
                logger.error(f"Failed to fetch page {page + 1}: {exc}")
                break
        
        logger.info(f"Collected {len(all_results)} results across {pages_fetched} pages")
        return all_results

    async def search_and_download(
        self, query: str, max_results: int, count_only: bool = False, full_recall: bool = False
    ) -> Tuple[List[Path], Optional[int], List[PatentSummary]]:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ê³  ìƒìœ„ Nê°œì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•œë‹¤.

        ë§¤ ì¿¼ë¦¬ë§ˆë‹¤ ìƒˆë¡œìš´ ë¸Œë¼ìš°ì €/ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ë³´ì•ˆ í† í°ì„ ì¬ìº¡ì²˜í•œë‹¤.
        """

        # ì¿¼ë¦¬ ë³„ ì§„ë‹¨ í´ë”
        diag_dir: Optional[Path] = None
        if self.diagnostics:
            qslug = self._slugify_for_path(query)
            diag_dir = self.diagnostics_dir / qslug

        async with async_playwright() as p:  # ì¿¼ë¦¬ë§ˆë‹¤ ìƒˆë¡œìš´ Playwright ì„¸ì…˜
            browser = await p.chromium.launch(headless=self.headless)
            context = await browser.new_context()
            page = await context.new_page()
            page.set_default_timeout(self.timeout * 1000)
            # per-query log íŒŒì¼ ì¶”ê°€ (ë™ì¼ í´ë” ë‚´)
            log_sink_id: Optional[int] = None
            try:
                log_path = (self.download_dir / "run.log").resolve()
                # ì¡´ì¬í•  ê²½ìš° ì´ì–´ì“°ê¸°
                log_sink_id = logger.add(str(log_path), level="INFO")
            except Exception:
                log_sink_id = None

            try:
                captured, page_html, xhr_text = await self._capture_xhr_request(
                    page, query, diag_dir=diag_dir
                )

                # httpx í´ë¼ì´ì–¸íŠ¸ êµ¬ì„±
                client = await self._build_client_with_cookies(context, captured)

                # XHR ìš°ì„  ì‹œë„: ë¨¼ì € ë¸Œë¼ìš°ì €ì—ì„œ ë°›ì€ ì›ë¬¸ XHR ì‘ë‹µìœ¼ë¡œ íŒŒì‹±
                results: List[PatentSummary] = []
                total_count: Optional[int] = None
                if xhr_text:
                    results, total_count = self._parse_results_from_xhr(xhr_text)
                    logger.info(f"Initial XHR results: {len(results)}")
                    if total_count is not None:
                        logger.info(f"Total search results available: {total_count}")

                # í•„ìš” ì‹œ ìº¡ì²˜ëœ ìš”ì²­ìœ¼ë¡œ httpx ì¬í˜„
                if not results and captured and "/xhr/query" in captured.url:
                    logger.info("Replaying captured XHR query via httpx ...")
                    try:
                        # ìº¡ì²˜ëœ í—¤ë” ì¤‘ httpxë¡œ ì „ë‹¬í•´ë„ ì•ˆì „í•œ í—¤ë”ë§Œ ì„ ë³„ ì „ë‹¬
                        banned = {
                            "cookie",
                            "host",
                            "authority",
                            "method",
                            "path",
                            "scheme",
                            "content-length",
                            "origin",
                        }
                        replay_headers = {
                            k.title(): v
                            for k, v in captured.headers.items()
                            if k.lower() not in banned
                        }
                        # ìµœì†Œ ìš”êµ¬ í—¤ë” ë³´ê°•
                        replay_headers.setdefault("X-Same-Domain", "1")
                        replay_headers.setdefault("Referer", captured.referer or GOOGLE_PATENTS_ORIGIN + "/")

                        resp = await client.get(captured.url, headers=replay_headers, timeout=10.0)
                        if self.diagnostics and diag_dir is not None:
                            (diag_dir / "xhr_query_response.html").write_text(
                                resp.text, encoding="utf-8"
                            )
                            (diag_dir / "captured_request.json").write_text(
                                json.dumps(
                                    {
                                        "url": captured.url,
                                        "headers": captured.headers,
                                        "referer": captured.referer,
                                    },
                                    indent=2,
                                    ensure_ascii=False,
                                ),
                                encoding="utf-8",
                            )

                        if resp.status_code < 400 and resp.text:
                            results, total_count = self._parse_results_from_xhr(resp.text)
                            logger.info(f"Replayed XHR results: {len(results)}")
                            if total_count is not None:
                                logger.info(f"Total search results available: {total_count}")
                        else:
                            logger.warning(
                                f"XHR {resp.status_code}; falling back to page HTML parse"
                            )
                    except Exception as exc:
                        logger.warning(f"XHR replay failed: {exc}")

                # í´ë°±: Playwrightë¡œ í™•ë³´í•œ í˜ì´ì§€ ì „ì²´ HTML íŒŒì‹±
                if not results:
                    results = self._parse_results_from_html(page_html)

                # ì¶”ê°€ í´ë°±: ê²€ìƒ‰ URLë¡œ ì§ì ‘ ì´ë™í•˜ì—¬ ë‹¤ì‹œ íŒŒì‹±
                if not results:
                    from urllib.parse import quote_plus
                    effective_query = self._normalize_query_string(query)
                    search_url = GOOGLE_PATENTS_ORIGIN + "/?q=" + quote_plus(effective_query) + "&hl=en&num=100"
                    try:
                        await page.goto(search_url)
                        await page.wait_for_load_state("domcontentloaded")
                        try:
                            await page.wait_for_load_state("networkidle", timeout=1500)
                        except Exception:
                            pass
                        await page.wait_for_timeout(250)
                        html2 = await page.content()
                        results = self._parse_results_from_html(html2)
                    except Exception:
                        pass

                # ìµœì¢… í´ë°±: DOM ì§ì ‘ íŒŒì‹±
                if not results:
                    try:
                        results = await self._parse_results_from_dom(page)
                    except Exception:
                        results = []

                # ì¶”ê°€ í˜ì´ì§•/ìŠ¤í¬ë¡¤: ë” ë§ì€ ê²°ê³¼ê°€ í•„ìš”í•˜ë©´ XHR ì¬ìš”ì²­, ìŠ¤í¬ë¡¤ ë¡œë“œ ë˜ëŠ” ë‹¤ìŒ í˜ì´ì§€ë¥¼ ë”°ë¼ê°€ë©° ìˆ˜ì§‘
                if len(results) < max_results:
                    seen: set[str] = {r.detail_url for r in results}

                    # 0) XHR ê¸°ë°˜ íŒŒë¼ë¯¸í„° í˜ì´ì§€ë„¤ì´ì…˜(ê°€ëŠ¥í•œ ê²½ìš°): page/start/num ì¡°í•© ì‹œë„
                    if captured and "/xhr/query" in captured.url:
                        try:
                            from urllib.parse import (
                                urlsplit,
                                urlunsplit,
                                parse_qsl,
                                urlencode,
                                unquote,
                            )

                            split = urlsplit(captured.url)
                            params = dict(parse_qsl(split.query, keep_blank_values=True))

                            # ìº¡ì²˜ëœ ì¿¼ë¦¬ëŠ” ìƒìœ„ íŒŒë¼ë¯¸í„° url= ì•ˆì— ì‹¤ì œ ì§ˆì˜ íŒŒë¼ë¯¸í„°ê°€ ì¡´ì¬í•¨
                            # ì˜ˆ: /xhr/query?url=q=...&oq=...
                            inner_raw = params.get("url", "")
                            inner_qs = unquote(inner_raw)
                            inner_params = dict(parse_qsl(inner_qs, keep_blank_values=True))
                            # í•œ í˜ì´ì§€ë‹¹ ìµœëŒ€í•œ ë§ì´ ê°€ì ¸ì˜¤ë„ë¡ ì‹œë„
                            inner_params.setdefault("num", "100")

                            def build_url(updated_inner: dict[str, str]) -> str:
                                outer = dict(params)
                                outer["url"] = urlencode(updated_inner)
                                return urlunsplit(
                                    (
                                        split.scheme,
                                        split.netloc,
                                        split.path,
                                        urlencode(outer),
                                        split.fragment,
                                    )
                                )

                            # ê³µí†µ ì¬ìƒ í—¤ë”(ìµœì†Œ ìš”êµ¬)
                            replay_headers = {
                                "X-Same-Domain": "1",
                                "Referer": captured.referer or GOOGLE_PATENTS_ORIGIN + "/",
                            }

                            # ìš°ì„  í˜„ì¬ íŒŒë¼ë¯¸í„°ë¡œ í•œ ë²ˆ ë” ìµœëŒ€ ê°œìˆ˜ ìš”ì²­ ì‹œë„
                            try:
                                inner_params["num"] = str(min(max_results, 100))
                                resp0 = await client.get(build_url(inner_params), headers=replay_headers, timeout=10.0)
                                if resp0.status_code < 400 and resp0.text:
                                    more0, _ = self._parse_results_from_xhr(resp0.text)
                                    for m in more0:
                                        if m.detail_url and m.detail_url not in seen:
                                            results.append(m)
                                            seen.add(m.detail_url)
                                            if len(results) >= max_results:
                                                break
                            except Exception:
                                pass

                            # page=2..N ì‹œë„
                            page_try_max = 10
                            if len(results) < max_results:
                                for page_no in range(2, page_try_max + 1):
                                    params_page = dict(inner_params)
                                    params_page["page"] = str(page_no)
                                    try:
                                        params_page["num"] = str(min(max_results, 100))
                                        resp = await client.get(build_url(params_page), headers=replay_headers, timeout=10.0)
                                        if resp.status_code >= 400 or not resp.text:
                                            break
                                        add_items, _ = self._parse_results_from_xhr(resp.text)
                                        new_added = 0
                                        for m in add_items:
                                            if m.detail_url and m.detail_url not in seen:
                                                results.append(m)
                                                seen.add(m.detail_url)
                                                new_added += 1
                                                if len(results) >= max_results:
                                                    break
                                        if new_added == 0:
                                            # ë™ì¼ ê²°ê³¼ë§Œ ë°˜ë³µë˜ë©´ ì¤‘ë‹¨
                                            break
                                    except Exception:
                                        break
                                    if len(results) >= max_results:
                                        break

                            # start=offset ì‹œë„(10 ë‹¨ìœ„)
                            if len(results) < max_results:
                                for start_offset in range(10, 1000, 10):
                                    params_start = dict(inner_params)
                                    params_start["start"] = str(start_offset)
                                    try:
                                        params_start["num"] = str(min(max_results, 100))
                                        resp = await client.get(build_url(params_start), headers=replay_headers, timeout=10.0)
                                        if resp.status_code >= 400 or not resp.text:
                                            break
                                        add_items, _ = self._parse_results_from_xhr(resp.text)
                                        new_added = 0
                                        for m in add_items:
                                            if m.detail_url and m.detail_url not in seen:
                                                results.append(m)
                                                seen.add(m.detail_url)
                                                new_added += 1
                                                if len(results) >= max_results:
                                                    break
                                        if new_added == 0:
                                            break
                                    except Exception:
                                        break
                                    if len(results) >= max_results:
                                        break
                        except Exception:
                            pass

                    async def collect_from_current_page() -> int:
                        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²°ê³¼ë¥¼ íŒŒì‹±í•´ resultsì— ë³‘í•©í•˜ê³  ìƒˆë¡œ ì¶”ê°€ëœ ê°œìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
                        added = 0
                        try:
                            html_now = await page.content()
                        except Exception:
                            html_now = ""
                        more = self._parse_results_from_html(html_now)
                        if not more:
                            try:
                                more = await self._parse_results_from_dom(page)
                            except Exception:
                                more = []
                        for item in more:
                            if item.detail_url and item.detail_url not in seen:
                                results.append(item)
                                seen.add(item.detail_url)
                                added += 1
                                if len(results) >= max_results:
                                    break
                        return added

                    # 1) ë¬´í•œ ìŠ¤í¬ë¡¤ í˜•íƒœ ì§€ì›: ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ ë” ë§ì€ articleì„ ë¡œë“œ
                    try:
                        while len(results) < max_results:
                            prev_len = len(results)
                            try:
                                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            except Exception:
                                break
                            try:
                                await page.wait_for_load_state("networkidle", timeout=1500)
                            except Exception:
                                pass
                            await page.wait_for_timeout(250)
                            added = await collect_from_current_page()
                            if added == 0 and len(results) == prev_len:
                                break
                    except Exception:
                        pass

                    # 2) ë‹¤ìŒ í˜ì´ì§€ ë§í¬ íƒìƒ‰: ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
                    next_selectors = [
                        "a[rel='next' i]",
                        "a[aria-label='Next' i]",
                        "a[aria-label*='Next' i]",
                        "a:has-text('Next')",
                        "a:has-text('ë‹¤ìŒ')",
                        "button:has-text('Next')",
                        "[role='link']:has-text('Next')",
                        "a#pnnext",
                        "a:has-text('â€º')",
                        "a[aria-label*='â€º']",
                    ]

                    while len(results) < max_results:
                        try:
                            next_locator = None
                            # ì…€ë ‰í„° í›„ë³´ë¥¼ ìˆœì„œëŒ€ë¡œ ê²€ì‚¬
                            for sel in next_selectors:
                                loc = page.locator(sel).first
                                try:
                                    if await loc.count():
                                        next_locator = loc
                                        break
                                except Exception:
                                    continue
                            if next_locator is None or not await next_locator.count():
                                break

                            href = await next_locator.get_attribute("href")
                            if not href:
                                # ë§í¬ê°€ ë²„íŠ¼ í˜•íƒœì¸ ê²½ìš° í´ë¦­ ì‹œë„
                                try:
                                    await next_locator.click()
                                except Exception:
                                    break
                                try:
                                    await page.wait_for_load_state("networkidle", timeout=1500)
                                except Exception:
                                    pass
                                await page.wait_for_timeout(250)
                            else:
                                next_url = href if href.startswith("http") else GOOGLE_PATENTS_ORIGIN + href
                                await page.goto(next_url)
                                try:
                                    await page.wait_for_load_state("networkidle", timeout=1500)
                                except Exception:
                                    pass
                                await page.wait_for_timeout(250)

                            added = await collect_from_current_page()
                            if added == 0:
                                # ìŠ¤í¬ë¡¤ ë³´ì¡° ì‹œë„ í›„ ì¢…ë£Œ
                                try:
                                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                except Exception:
                                    pass
                                await page.wait_for_timeout(200)
                                added2 = await collect_from_current_page()
                                if added2 == 0:
                                    break
                        except Exception:
                            break

                if not results:
                    logger.warning("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    return [], total_count, []

                results = results[:max_results]
                logger.info(f"Parsed {len(results)} results")

                # count_only ëª¨ë“œì—ì„œëŠ” PDF ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê³  íŠ¹í—ˆ ì •ë³´ë§Œ ë°˜í™˜
                if count_only:
                    # full_recall ëª¨ë“œì—ì„œëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
                    if full_recall and total_count and total_count > len(results):
                        logger.info(f"Full recall mode: fetching all {total_count} results...")
                        # target_patentì„ ì¸ìë¡œ ì „ë‹¬í•˜ì—¬ early termination í™œìš©
                        target_patent = getattr(self, '_target_patent', None)
                        results = await self._fetch_all_results(client, captured, total_count, replay_headers, target_patent)
                    return [], total_count, results

                saved: List[Path] = []
                saved_meta: List[Dict[str, Any]] = []
                for idx, item in enumerate(results, 1):
                    await asyncio.sleep(self.delay)
                    logger.info(
                        f"[{idx}/{len(results)}] {item.publication_number} â†’ detail"
                    )

                    pdf_url = await self._fetch_detail_and_pdf(client, item.detail_url)
                    if not pdf_url:
                        logger.warning("PDF URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    base_name = item.publication_number or f"Patent_{idx}"
                    base_name = self._sanitize_filename(base_name)
                    out_path = self.download_dir / f"{base_name}.pdf"

                    ok = await self._download_pdf(
                        client, pdf_url, out_path, referer=item.detail_url
                    )
                    if ok:
                        size = out_path.stat().st_size
                        logger.info(f"âœ… Saved {out_path.name} ({size:,} bytes)")
                        saved.append(out_path)
                        saved_meta.append({
                            "publication_number": item.publication_number,
                            "title": item.title,
                            "detail_url": item.detail_url,
                            "pdf_url": pdf_url,
                            "saved_path": str(out_path.resolve()),
                            "size_bytes": size,
                        })
                    else:
                        logger.error(f"âŒ Failed to save {out_path.name}")

                # ì¿¼ë¦¬ ë©”íƒ€ë°ì´í„° ì €ì¥
                try:
                    meta = {
                        "query": query,
                        "effective_query": self._normalize_query_string(query),
                        "timestamp": datetime.now().isoformat(),
                        "download_dir": str(self.download_dir.resolve()),
                        "count": len(saved_meta),
                        "items": saved_meta,
                    }
                    (self.download_dir / "query.json").write_text(
                        json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
                    )
                    (self.download_dir / "query.txt").write_text(query, encoding="utf-8")
                except Exception:
                    pass

                return saved, total_count, results
            finally:
                # httpx í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
                try:
                    if 'client' in locals():
                        await client.aclose()
                except Exception:
                    pass
                await context.close()
                await browser.close()
                # ë¡œê·¸ sink ì œê±°
                if log_sink_id is not None:
                    try:
                        logger.remove(log_sink_id)
                    except Exception:
                        pass

    async def search_and_download_many(
        self, queries: List[str], max_results: int, count_only: bool = False, full_recall: bool = False
    ) -> Dict[str, Tuple[List[Path], Optional[int], List[PatentSummary]]]:
        """ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ìˆœì°¨ ì²˜ë¦¬. ê° ì¿¼ë¦¬ë§ˆë‹¤ ë³´ì•ˆ í† í°ì„ ì¬ìº¡ì²˜í•œë‹¤."""

        results: Dict[str, Tuple[List[Path], Optional[int], List[PatentSummary]]] = {}
        for i, q in enumerate(queries, 1):
            logger.info(f"â—‡ Query {i}/{len(queries)}: {q}")
            try:
                saved, total_count, patents = await self.search_and_download(q, max_results, count_only, full_recall)
                results[q] = (saved, total_count, patents)
            except Exception as exc:
                logger.error(f"Query failed: {q} ({exc})")
                results[q] = ([], None, [])
            if i < len(queries):
                await asyncio.sleep(max(self.delay, 0.5))
        return results

    @staticmethod
    def _slugify_for_path(text: str) -> str:
        slug = re.sub(r"\s+", "_", text.strip())
        slug = re.sub(r"[^A-Za-z0-9_\-]+", "", slug)
        return slug[:50] or "query"


def _build_cli_parser() -> Any:
    import argparse

    parser = argparse.ArgumentParser(
        description="Google Patents XHR + httpx ê¸°ë°˜ PDF ë‹¤ìš´ë¡œë”"
    )
    parser.add_argument(
        "--query",
        action="append",
        help="ê²€ìƒ‰ì–´ (ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥)",
    )
    parser.add_argument(
        "--query-file",
        help="ì¿¼ë¦¬ ëª©ë¡ íŒŒì¼(ì¤„ë‹¨ìœ„)",
    )
    parser.add_argument(
        "--out",
        required=True,
        help="PDF ì €ì¥ ë””ë ‰í„°ë¦¬ ì ˆëŒ€ê²½ë¡œ",
    )
    parser.add_argument(
        "--max-results", type=int, default=5, help="ìµœëŒ€ ë‹¤ìš´ë¡œë“œ ìˆ˜"
    )
    parser.add_argument(
        "--delay", type=float, default=1.0, help="ë‹¤ìš´ë¡œë“œ ì‚¬ì´ ëŒ€ê¸°(ì´ˆ)"
    )
    parser.add_argument(
        "--timeout", type=int, default=30, help="ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ(ì´ˆ)"
    )
    parser.add_argument(
        "--headless", action="store_true", help="ë¸Œë¼ìš°ì € í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ"
    )
    parser.add_argument(
        "--diagnostics",
        action="store_true",
        help="XHR/HTML ì§„ë‹¨ ì•„í‹°íŒ©íŠ¸ ì €ì¥",
    )
    parser.add_argument(
        "--count-only",
        action="store_true",
        help="ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë§Œ í™•ì¸ (PDF ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°)",
    )
    return parser


async def _amain(argv: List[str]) -> int:
    parser = _build_cli_parser()
    args = parser.parse_args(argv)

    out_dir = Path(args.out).expanduser().resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    downloader = GooglePatentsXHRDownloader(
        download_dir=out_dir,
        headless=args.headless,
        timeout=args.timeout,
        delay=args.delay,
        diagnostics=args.diagnostics,
    )

    # ì¿¼ë¦¬ ìˆ˜ì§‘
    queries: List[str] = []
    if args.query:
        queries.extend([q for q in args.query if q and q.strip()])
    if args.query_file:
        qpath = Path(args.query_file).expanduser().resolve()
        if not qpath.exists():
            raise SystemExit(f"query-file not found: {qpath}")
        for line in qpath.read_text(encoding="utf-8").splitlines():
            if line.strip():
                queries.append(line.strip())

    if not queries:
        raise SystemExit("--query ë˜ëŠ” --query-file ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤.")

    if len(queries) == 1:
        saved, total_count, patents = await downloader.search_and_download(
            query=queries[0], max_results=args.max_results, count_only=args.count_only
        )
        if args.count_only:
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {queries[0]}")
            print(f"   - íŒŒì‹±ëœ ê²°ê³¼: {len(patents)} ê±´")
            if total_count is not None:
                print(f"   - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {total_count:,} ê±´")
            else:
                print("   - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: ì•Œ ìˆ˜ ì—†ìŒ")
        else:
            logger.info(f"ì´ {len(saved)}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {out_dir}")
    else:
        all_saved = await downloader.search_and_download_many(
            queries=queries, max_results=args.max_results, count_only=args.count_only
        )
        if args.count_only:
            print(f"ğŸ“Š ì—¬ëŸ¬ ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼:")
            for query, (files, total_count, patents) in all_saved.items():
                print(f"   ğŸ” {query}")
                print(f"      - íŒŒì‹±ëœ ê²°ê³¼: {len(patents)} ê±´")
                if total_count is not None:
                    print(f"      - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {total_count:,} ê±´")
                else:
                    print("      - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: ì•Œ ìˆ˜ ì—†ìŒ")
        else:
            total = sum(len(files) for files, _, _ in all_saved.values())
            logger.info(f"ì´ {total}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ ({len(queries)}ê°œ ì¿¼ë¦¬): {out_dir}")
    return 0


def main() -> None:
    try:
        rc = asyncio.run(_amain(sys.argv[1:]))
        raise SystemExit(rc)
    except KeyboardInterrupt:
        raise SystemExit(130)


if __name__ == "__main__":
    main()
