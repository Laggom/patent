"""íŠ¹í—ˆ ë¶„ì„ ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸

PDF íŠ¹í—ˆ ì…ë ¥ë¶€í„° Seed Recall ë¶„ì„ê¹Œì§€ ì›í´ë¦­ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  python patent_pipeline.py --pdf patent.pdf --output full_analysis.json
  
  # ì „ì²´ Seed Recall ê³„ì‚° (ë” ì •í™•í•˜ì§€ë§Œ ì˜¤ë˜ ê±¸ë¦¼)
  python patent_pipeline.py --pdf patent.pdf --full-recall --output detailed_analysis.json
  
  # ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
  python patent_pipeline.py --pdf patent.pdf --prompt custom_prompt.txt --delay 2.0 --max-results 20

ì›Œí¬í”Œë¡œìš°:
1. PDF ë¶„ì„ â†’ Gemini API â†’ ê²€ìƒ‰ì‹ ìƒì„± (query_generator.py)
2. ê²€ìƒ‰ì‹ â†’ Google Patents ê²€ìƒ‰ â†’ ê²°ê³¼ ìˆ˜ì§‘ (recall_analyzer.py)  
3. Seed Recall ê³„ì‚° ë° í†µí•© ê²°ê³¼ ìƒì„±

ì£¼ìš” ê¸°ëŠ¥:
- ë©”ëª¨ë¦¬ìƒ ë°ì´í„° ì „ë‹¬ë¡œ ì¤‘ê°„ íŒŒì¼ ìƒì„± ìµœì†Œí™”
- ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ë¡œê¹…
- ì—ëŸ¬ ë°œìƒì‹œ ì¤‘ë‹¨ì ë¶€í„° ì¬ì‹œì‘ ê°€ëŠ¥
- ê¸°ì¡´ ê°œë³„ ëª¨ë“ˆë“¤ì˜ ëª¨ë“  ì˜µì…˜ ì§€ì›
- í†µí•© JSON ê²°ê³¼ + ê°œë³„ ë‹¨ê³„ ê²°ê³¼ ì €ì¥

í•„ìˆ˜ ì„¤ì •:
1. pip install -r requirements.txt
2. python -m playwright install  
3. Google AI Studioì—ì„œ API í‚¤ ë°œê¸‰
4. .env íŒŒì¼ì— GOOGLE_API_KEY ì„¤ì • ë˜ëŠ” --api-key ì‚¬ìš©
"""

import argparse
import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from dotenv import load_dotenv
from loguru import logger

# ê¸°ì¡´ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from query_generator import PatentQueryGenerator, load_prompt_template, extract_patent_number_from_filename
from recall_analyzer import RecallAnalyzer
from prompt_manager import PromptManager, create_default_prompt_manager


class PatentAnalysisPipeline:
    """íŠ¹í—ˆ ë¶„ì„ ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        api_key: str,
        download_dir: Path = Path("./temp_downloads"),
        max_results: int = 10,
        delay: float = 1.5,
        full_recall: bool = False,
        save_intermediate: bool = True,
        prompt_manager: Optional[PromptManager] = None
    ):
        self.api_key = api_key
        self.download_dir = download_dir
        self.max_results = max_results
        self.delay = delay
        self.full_recall = full_recall
        self.save_intermediate = save_intermediate
        
        # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.prompt_manager = prompt_manager or create_default_prompt_manager()
        
        # ëª¨ë“ˆë“¤ ì´ˆê¸°í™”
        self.query_generator = PatentQueryGenerator(api_key)
        self.recall_analyzer = RecallAnalyzer(
            download_dir=download_dir,
            max_results=max_results,
            delay=delay,
            full_recall=full_recall
        )
        
        logger.info(f"Pipeline initialized (full_recall={full_recall}, max_results={max_results})")
    
    async def run_complete_analysis(
        self,
        pdf_path: Path,
        prompt_template: str,
        output_dir: Path = Path("./temp_results")
    ) -> Dict[str, Any]:
        """ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"Starting complete patent analysis pipeline for: {pdf_path.name}")
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # íŠ¹í—ˆ ë²ˆí˜¸ ì¶”ì¶œ
        patent_number = extract_patent_number_from_filename(pdf_path)
        
        try:
            # STEP 1: ê²€ìƒ‰ì‹ ìƒì„±
            logger.info("STEP 1: Generating search queries with AI...")
            queries_data = await self.query_generator.generate_queries_from_pdf(
                pdf_path, prompt_template
            )
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì„ íƒì )
            if self.save_intermediate:
                queries_file = output_dir / f"{patent_number}_queries.json"
                self._save_json(queries_data, queries_file)
                logger.info(f"Queries saved to: {queries_file}")
            
            # ìƒì„±ëœ ê²€ìƒ‰ì‹ ìˆ˜ í™•ì¸
            search_queries = queries_data.get("search_queries", [])
            logger.info(f"Generated {len(search_queries)} search queries")
            
            # STEP 2: Seed Recall ë¶„ì„
            logger.info("STEP 2: Executing searches and analyzing recall...")
            recall_results = await self.recall_analyzer.analyze_recall(
                queries_data, patent_number
            )
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì„ íƒì )
            if self.save_intermediate:
                recall_file = output_dir / f"{patent_number}_recall.json"
                self._save_json(recall_results, recall_file)
                logger.info(f"Recall analysis saved to: {recall_file}")
            
            # STEP 3: ê²°ê³¼ í†µí•©
            logger.info("STEP 3: Combining results...")
            integrated_result = self._integrate_results(
                pdf_path, queries_data, recall_results
            )
            
            # ì„±ê³µ ë¡œê¹…
            performance = recall_results.get("performance_analysis", {})
            logger.info(f"Pipeline completed successfully!")
            logger.info(f"Seed Recall Rate: {performance.get('seed_recall_rate', 0):.2%}")
            logger.info(f"Successful queries: {performance.get('successful_queries', 0)}/{performance.get('total_queries', 0)}")
            
            # ì„±ê³¼ ê¸°ë¡ (í”„ë¡¬í”„íŠ¸ ì „ëµì´ ìˆëŠ” ê²½ìš°)
            strategy = getattr(self, '_current_strategy', None)
            if strategy and strategy != 'custom':
                prompt_manager = PromptManager()
                prompt_manager.record_performance(
                    strategy=strategy,
                    patent_number=patent_number,
                    performance_data={
                        "seed_recall_rate": performance.get("seed_recall_rate", 0),
                        "total_queries": performance.get("total_queries", 0),
                        "successful_queries": performance.get("successful_queries", 0),
                        "queries_found_patent": performance.get("queries_found_patent", 0),
                        "timestamp": datetime.now().isoformat()
                    }
                )
            
            return integrated_result
            
        except Exception as exc:
            logger.error(f"Complete analysis pipeline failed: {exc}")
            raise
    
    async def run_multi_prompt_analysis(
        self,
        pdf_path: Path,
        strategies: List[str],
        output_dir: Path = Path("./temp_results")
    ) -> Dict[str, Any]:
        """ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ë¡œ ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰"""
        logger.info(f"Starting multi-prompt analysis for: {pdf_path.name}")
        logger.info(f"Strategies: {strategies}")
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        output_dir.mkdir(parents=True, exist_ok=True)
        patent_number = extract_patent_number_from_filename(pdf_path)
        
        # ê° ì „ëµë³„ ê²°ê³¼ ì €ì¥
        strategy_results = {}
        best_result = None
        best_recall_rate = -1.0
        
        try:
            for i, strategy in enumerate(strategies, 1):
                logger.info(f"STRATEGY {i}/{len(strategies)}: {strategy}")
                
                # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
                try:
                    prompt_template = self.prompt_manager.get_prompt(strategy)
                except Exception as exc:
                    logger.error(f"Failed to load prompt '{strategy}': {exc}")
                    continue
                
                # 1. ê²€ìƒ‰ì‹ ìƒì„±
                logger.info(f"Generating queries with {strategy} strategy...")
                queries_data = await self.query_generator.generate_queries_from_pdf(
                    pdf_path, prompt_template
                )
                
                # 2. Recall ë¶„ì„
                logger.info(f"Analyzing recall for {strategy}...")
                recall_results = await self.recall_analyzer.analyze_recall(
                    queries_data, patent_number
                )
                
                # 3. ê²°ê³¼ í†µí•© (ë‹¨ì¼ ì „ëµ ê²°ê³¼)
                integrated_result = self._integrate_results(
                    pdf_path, queries_data, recall_results
                )
                
                strategy_results[strategy] = integrated_result
                
                # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                if self.save_intermediate:
                    result_file = output_dir / f"{patent_number}_{strategy}.json"
                    self._save_json(integrated_result, result_file)
                    logger.info(f"Strategy result saved: {result_file}")
                
                # ìµœê³  ì„±ê³¼ ì¶”ì 
                performance = recall_results.get("performance_analysis", {})
                recall_rate = performance.get("seed_recall_rate", 0)
                
                if recall_rate > best_recall_rate:
                    best_recall_rate = recall_rate
                    best_result = integrated_result
                    
                # í”„ë¡¬í”„íŠ¸ ì„±ê³¼ ê¸°ë¡
                self.prompt_manager.record_performance(
                    strategy, patent_number, performance
                )
                
                logger.info(f"Strategy {strategy}: Seed Recall = {recall_rate:.2%}")
                
                # ì „ëµê°„ ì§€ì—°
                if i < len(strategies):
                    await asyncio.sleep(1.0)
                    
            # í†µí•© ê²°ê³¼ ìƒì„±
            # ì „ëµë³„ ì¿¼ë¦¬ ë°ì´í„°ì™€ ë¦¬ì½œ ê²°ê³¼ ë¶„ë¦¬
            multi_queries_data = {}
            multi_recall_results = {}
            
            for strategy, result in strategy_results.items():
                multi_queries_data[strategy] = result.get("query_generation", {})
                # ë¦¬ì½œ ê²°ê³¼ëŠ” ì „ì²´ ì„±ê³¼ ë¶„ì„ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬
                multi_recall_results[strategy] = {
                    "performance_analysis": result.get("performance_analysis", {}),
                    "search_results": result.get("search_execution", {}).get("search_results", [])
                }
                
            multi_result = self._integrate_results_multi(
                pdf_path, multi_queries_data, multi_recall_results
            )
            
            # ìµœê³  ì„±ê³¼ ì „ëµ ê¸°ë¡
            if best_result:
                best_strategy = None
                for strategy, result in strategy_results.items():
                    recall_rate = result.get("performance_analysis", {}).get("seed_recall_rate", 0)
                    if recall_rate == best_recall_rate:
                        best_strategy = strategy
                        break
                        
                multi_result["metadata"]["best_strategy"] = best_strategy
                multi_result["best_result"] = best_result
                
            logger.info(f"Multi-prompt analysis completed!")
            logger.info(f"Best strategy: {best_strategy} (Recall: {best_recall_rate:.2%})")
            
            return multi_result
            
        except Exception as exc:
            logger.error(f"Multi-prompt analysis failed: {exc}")
            raise
            
        except Exception as exc:
            logger.error(f"âŒ Pipeline failed: {exc}")
            
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜
            partial_result = {
                "metadata": {
                    "pdf_file": str(pdf_path),
                    "patent_number": patent_number,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "pipeline_status": "failed",
                    "error": str(exc)
                },
                "queries_data": locals().get("queries_data", {}),
                "recall_results": locals().get("recall_results", {}),
                "integrated_analysis": {}
            }
            
            return partial_result
    
    def _integrate_results(
        self, 
        pdf_path: Path, 
        queries_data: Dict[str, Any], 
        recall_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ì‹ ìƒì„± ê²°ê³¼ì™€ Recall ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©"""
        patent_number = extract_patent_number_from_filename(pdf_path)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance = recall_results.get("performance_analysis", {})
        search_results = recall_results.get("search_results", [])
        
        # ê° ê²€ìƒ‰ì‹ë³„ ì„±ê³¼ ìš”ì•½
        query_performance = []
        for detail in performance.get("query_details", []):
            query_performance.append({
                "strategy": detail.get("strategy", "Unknown"),
                "query": detail.get("query", ""),
                "found_target": detail.get("found", False),
                "total_results": detail.get("total_results", 0),
                "parsed_results": detail.get("parsed_results", 0)
            })
        
        # í†µí•© ê²°ê³¼ êµ¬ì„±
        integrated_result = {
            "metadata": {
                "pdf_file": str(pdf_path),
                "patent_number": patent_number,
                "analysis_timestamp": datetime.now().isoformat(),
                "pipeline_version": "1.0",
                "pipeline_status": "completed",
                "settings": {
                    "max_results": self.max_results,
                    "delay": self.delay,
                    "full_recall": self.full_recall
                }
            },
            
            # 1ë‹¨ê³„: ê²€ìƒ‰ì‹ ìƒì„± ê²°ê³¼
            "query_generation": {
                "patent_info": queries_data.get("patent_info", {}),
                "search_queries": queries_data.get("search_queries", []),
                "generation_metadata": queries_data.get("metadata", {})
            },
            
            # 2ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰ ê²°ê³¼
            "search_execution": {
                "search_results": search_results,
                "execution_metadata": recall_results.get("metadata", {})
            },
            
            # 3ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
            "performance_analysis": {
                **performance,
                "query_performance": query_performance
            },
            
            # ì¢…í•© ê²°ë¡ 
            "summary": {
                "total_queries_generated": len(queries_data.get("search_queries", [])),
                "successful_searches": performance.get("successful_queries", 0),
                "seed_recall_rate": performance.get("seed_recall_rate", 0),
                "queries_found_target": performance.get("queries_found_patent", 0),
                "avg_results_per_query": performance.get("avg_parsed_results", 0),
                "analysis_success": performance.get("seed_recall_rate", 0) > 0
            }
        }
        
        return integrated_result
    
    def _integrate_results_multi(
        self, 
        pdf_path: Path, 
        multi_queries_data: Dict[str, Dict[str, Any]], 
        multi_recall_results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ê²°ê³¼ í†µí•©"""
        patent_number = extract_patent_number_from_filename(pdf_path)
        
        # ì „ëµë³„ ì„±ê³¼ ìš”ì•½
        strategy_performances = {}
        all_queries = []
        
        for strategy, recall_data in multi_recall_results.items():
            performance = recall_data.get("performance_analysis", {})
            queries_data = multi_queries_data.get(strategy, {})
            
            strategy_performances[strategy] = {
                "strategy_name": strategy,
                "total_queries": len(queries_data.get("search_queries", [])),
                "successful_searches": performance.get("successful_queries", 0),
                "seed_recall_rate": performance.get("seed_recall_rate", 0),
                "queries_found_target": performance.get("queries_found_patent", 0),
                "avg_results_per_query": performance.get("avg_parsed_results", 0)
            }
            
            # ëª¨ë“  ì¿¼ë¦¬ ìˆ˜ì§‘
            for query_detail in performance.get("query_details", []):
                all_queries.append({
                    **query_detail,
                    "strategy": strategy
                })
        
        # ìµœê³  ì„±ê³¼ ì „ëµ ì‹ë³„
        best_strategy, best_performance = self._get_best_strategy(strategy_performances)
        
        # ì „ëµ ë¹„êµ ë¶„ì„
        comparison = self._compare_strategies(strategy_performances)
        
        # ì¶”ì²œ ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(strategy_performances, comparison)
        
        # í†µí•© ê²°ê³¼ êµ¬ì„±
        integrated_result = {
            "metadata": {
                "pdf_file": str(pdf_path),
                "patent_number": patent_number,
                "analysis_timestamp": datetime.now().isoformat(),
                "pipeline_version": "1.0-multi",
                "pipeline_status": "completed",
                "analysis_type": "multi_prompt",
                "strategies_used": list(multi_queries_data.keys()),
                "settings": {
                    "max_results": self.max_results,
                    "delay": self.delay,
                    "full_recall": self.full_recall
                }
            },
            
            # ì „ëµë³„ ìƒì„¸ ê²°ê³¼
            "strategy_results": {
                strategy: {
                    "query_generation": multi_queries_data.get(strategy, {}),
                    "recall_analysis": recall_data,
                    "performance_summary": strategy_performances[strategy]
                }
                for strategy, recall_data in multi_recall_results.items()
            },
            
            # ì „ëµ ë¹„êµ ë° ë¶„ì„
            "strategy_comparison": {
                "best_strategy": {
                    "name": best_strategy,
                    "performance": best_performance
                },
                "performance_ranking": comparison["ranking"],
                "comparative_analysis": comparison["analysis"],
                "recommendations": recommendations
            },
            
            # ì¢…í•© ì„±ê³¼ ìš”ì•½  
            "summary": {
                "total_strategies": len(multi_queries_data),
                "total_queries_generated": sum(len(data.get("search_queries", [])) for data in multi_queries_data.values()),
                "best_seed_recall_rate": best_performance.get("seed_recall_rate", 0),
                "strategies_found_target": sum(1 for perf in strategy_performances.values() if perf["queries_found_target"] > 0),
                "analysis_success": best_performance.get("seed_recall_rate", 0) > 0,
                "recommended_strategy": recommendations.get("primary_recommendation")
            }
        }
        
        return integrated_result
    
    def _get_best_strategy(self, strategy_performances: Dict[str, Dict[str, Any]]) -> Tuple[str, Dict[str, Any]]:
        """ìµœê³  ì„±ê³¼ ì „ëµ ì‹ë³„"""
        if not strategy_performances:
            return "none", {}
        
        # 1ì°¨: Seed Recall Rate ê¸°ì¤€
        # 2ì°¨: íƒ€ê²Ÿ ë°œê²¬ ì¿¼ë¦¬ ìˆ˜ ê¸°ì¤€
        # 3ì°¨: ì„±ê³µ ì¿¼ë¦¬ ìˆ˜ ê¸°ì¤€
        best_strategy = max(
            strategy_performances.items(),
            key=lambda x: (
                x[1]["seed_recall_rate"],
                x[1]["queries_found_target"],
                x[1]["successful_searches"]
            )
        )
        
        return best_strategy[0], best_strategy[1]
    
    def _compare_strategies(self, strategy_performances: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ì „ëµê°„ ì„±ê³¼ ë¹„êµ ë¶„ì„"""
        # ì„±ê³¼ìˆœ ìˆœìœ„
        ranking = sorted(
            strategy_performances.items(),
            key=lambda x: (
                x[1]["seed_recall_rate"],
                x[1]["queries_found_target"],
                x[1]["successful_searches"]
            ),
            reverse=True
        )
        
        # ì„±ê³¼ ë¶„ì„
        recall_rates = [perf["seed_recall_rate"] for perf in strategy_performances.values()]
        target_found_counts = [perf["queries_found_target"] for perf in strategy_performances.values()]
        
        analysis = {
            "recall_rate_range": {
                "min": min(recall_rates) if recall_rates else 0,
                "max": max(recall_rates) if recall_rates else 0,
                "avg": sum(recall_rates) / len(recall_rates) if recall_rates else 0
            },
            "strategies_found_target": sum(1 for count in target_found_counts if count > 0),
            "most_effective": ranking[0][0] if ranking else None,
            "performance_gap": (max(recall_rates) - min(recall_rates)) if recall_rates else 0
        }
        
        return {
            "ranking": [(name, perf) for name, perf in ranking],
            "analysis": analysis
        }
    
    def _generate_recommendations(
        self, 
        strategy_performances: Dict[str, Dict[str, Any]], 
        comparison: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ëµë³„ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        analysis = comparison["analysis"]
        ranking = comparison["ranking"]
        
        recommendations = {
            "primary_recommendation": ranking[0][0] if ranking else None,
            "reasoning": [],
            "alternative_strategies": [],
            "optimization_suggestions": []
        }
        
        if not ranking:
            return recommendations
        
        best_strategy, best_perf = ranking[0]
        
        # 1ì°¨ ì¶”ì²œ ê·¼ê±°
        if best_perf["seed_recall_rate"] > 0:
            recommendations["reasoning"].append(
                f"{best_strategy}ê°€ {best_perf['seed_recall_rate']:.1%} Seed Recall Rateë¡œ ìµœê³  ì„±ê³¼"
            )
        else:
            recommendations["reasoning"].append(
                "ëª¨ë“  ì „ëµì—ì„œ íƒ€ê²Ÿ íŠ¹í—ˆë¥¼ ë°œê²¬í•˜ì§€ ëª»í•¨. ê²€ìƒ‰ ì¡°ê±´ ì¬ê²€í†  í•„ìš”"
            )
        
        # ëŒ€ì•ˆ ì „ëµ ì œì•ˆ
        if len(ranking) > 1:
            second_best = ranking[1]
            if second_best[1]["seed_recall_rate"] > 0:
                recommendations["alternative_strategies"].append({
                    "strategy": second_best[0],
                    "reason": f"ë‘ ë²ˆì§¸ ë†’ì€ ì„±ê³¼ ({second_best[1]['seed_recall_rate']:.1%})"
                })
        
        # ìµœì í™” ì œì•ˆ
        if analysis["performance_gap"] > 0.2:  # 20% ì´ìƒ ì°¨ì´
            recommendations["optimization_suggestions"].append(
                "ì „ëµê°„ ì„±ê³¼ ì°¨ì´ê°€ í¼. ìµœê³  ì„±ê³¼ ì „ëµ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ê³ ë ¤"
            )
        
        if analysis["strategies_found_target"] < len(strategy_performances) / 2:
            recommendations["optimization_suggestions"].append(
                "ëŒ€ë¶€ë¶„ ì „ëµì—ì„œ íƒ€ê²Ÿ ë¯¸ë°œê²¬. ê¸°ë³¸ ê²€ìƒ‰ ì¡°ê±´ ì™„í™” ê¶Œì¥"
            )
        
        return recommendations
    
    def _save_json(self, data: Dict[str, Any], file_path: Path) -> None:
        """JSON ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


def setup_api_key(args) -> str:
    """API í‚¤ ì„¤ì •"""
    if args.api_key:
        return args.api_key
    
    load_dotenv()
    api_key = os.getenv('GOOGLE_API_KEY')
    
    if not api_key:
        raise ValueError(
            "Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. "
            "1) --api-key ì˜µì…˜ ì‚¬ìš© ë˜ëŠ” "
            "2) .env íŒŒì¼ì— GOOGLE_API_KEY ì„¤ì •"
        )
    
    return api_key


def build_cli_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ êµ¬ì„±"""
    parser = argparse.ArgumentParser(
        description="íŠ¹í—ˆ PDF ì™„ì „ ìë™í™” ë¶„ì„ íŒŒì´í”„ë¼ì¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    # í•„ìˆ˜ ì¸ì
    parser.add_argument(
        "--pdf", 
        type=Path,
        required=True,
        help="ë¶„ì„í•  íŠ¹í—ˆ PDF íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--output", 
        type=Path,
        required=True,
        help="ìµœì¢… ê²°ê³¼ ì €ì¥ ê²½ë¡œ (JSON íŒŒì¼)"
    )
    
    # ì„ íƒì  ì¸ì
    # í”„ë¡¬í”„íŠ¸ ì „ëµ ì„ íƒ (ë‹¨ì¼ ë˜ëŠ” ë‹¤ì¤‘)
    prompt_group = parser.add_mutually_exclusive_group()
    
    prompt_group.add_argument(
        "--prompt",
        type=Path,
        help="ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ ê²½ë¡œ"
    )
    
    prompt_group.add_argument(
        "--strategy",
        type=str,
        choices=["base_template", "technical_depth", "application_focus", 
                "competitor_analysis", "prior_art", "evolution_tracking", "auto"],
        help="í”„ë¡¬í”„íŠ¸ ì „ëµ ì„ íƒ (auto: ìë™ ì„ íƒ)"
    )
    
    prompt_group.add_argument(
        "--multi-strategy",
        nargs='+',
        choices=["base_template", "technical_depth", "application_focus", 
                "competitor_analysis", "prior_art", "evolution_tracking"],
        help="ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ ì „ëµ ë³‘ë ¬ ì‹¤í–‰ (ì˜ˆ: --multi-strategy technical_depth prior_art)"
    )
    
    parser.add_argument(
        "--api-key",
        type=str,
        help="Google Gemini API í‚¤ (í™˜ê²½ë³€ìˆ˜ GOOGLE_API_KEY ëŒ€ì‹  ì‚¬ìš©)"
    )
    
    parser.add_argument(
        "--download-dir",
        type=Path,
        default=Path("./temp_downloads"),
        help="ì„ì‹œ ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: ./temp_downloads)"
    )
    
    parser.add_argument(
        "--max-results",
        type=int,
        default=10,
        help="ê²€ìƒ‰ì‹ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )
    
    parser.add_argument(
        "--delay",
        type=float,
        default=1.5,
        help="ê²€ìƒ‰ ê°„ ì§€ì—°ì‹œê°„(ì´ˆ) (ê¸°ë³¸ê°’: 1.5)"
    )
    
    parser.add_argument(
        "--full-recall",
        action="store_true",
        help="ì „ì²´ ê²€ìƒ‰ ìˆ˜í–‰ (early termination ë¹„í™œì„±í™”, ë” ì •í™•í•˜ì§€ë§Œ ì˜¤ë˜ ê±¸ë¦¼)"
    )
    
    parser.add_argument(
        "--no-intermediate",
        action="store_true",
        help="ì¤‘ê°„ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì•ˆí•¨ (ìµœì¢… ê²°ê³¼ë§Œ ì €ì¥)"
    )
    
    return parser


async def main() -> int:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = build_cli_parser()
    args = parser.parse_args()
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not args.pdf.exists():
            logger.error(f"PDF file not found: {args.pdf}")
            return 1
        
        # API í‚¤ ì„¤ì •
        api_key = setup_api_key(args)
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = PatentAnalysisPipeline(
            api_key=api_key,
            download_dir=args.download_dir,
            max_results=args.max_results,
            delay=args.delay,
            full_recall=args.full_recall,
            save_intermediate=not args.no_intermediate
        )
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •
        output_dir = args.output.parent
        if output_dir != Path("."):
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # í”„ë¡¬í”„íŠ¸ ì „ëµì— ë”°ë¥¸ ì‹¤í–‰ ë¶„ê¸°
        if args.multi_strategy:
            # ë‹¤ì¤‘ ì „ëµ ë³‘ë ¬ ì‹¤í–‰
            logger.info(f"ğŸ¯ Starting multi-strategy analysis with: {', '.join(args.multi_strategy)}")
            results = await pipeline.run_multi_prompt_analysis(
                args.pdf, 
                args.multi_strategy,
                output_dir
            )
            
            # ë‹¤ì¤‘ ì „ëµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            summary = results.get("summary", {})
            metadata = results.get("metadata", {})
            comparison = results.get("strategy_comparison", {})
            
            print(f"ë‹¤ì¤‘ ì „ëµ ë¶„ì„ ì™„ë£Œ!")
            print(f"íŠ¹í—ˆ: {metadata.get('patent_number', 'Unknown')}")
            print(f"ì‹¤í–‰ ì „ëµ: {summary.get('total_strategies', 0)}ê°œ")
            print(f"ì´ ê²€ìƒ‰ì‹: {summary.get('total_queries_generated', 0)}ê°œ")
            print(f"ìµœê³  Seed Recall Rate: {summary.get('best_seed_recall_rate', 0):.2%}")
            print(f"ì¶”ì²œ ì „ëµ: {summary.get('recommended_strategy', 'None')}")
            
            if comparison.get("best_strategy"):
                best = comparison["best_strategy"]
                print(f"ìµœê³  ì„±ê³¼ ì „ëµ: {best['name']} ({best['performance'].get('seed_recall_rate', 0):.2%})")
            
        else:
            # ë‹¨ì¼ ì „ëµ ì‹¤í–‰
            prompt_manager = PromptManager()
            
            if args.strategy:
                if args.strategy == "auto":
                    # PDF ë¶„ì„ í›„ ìë™ ì„ íƒ
                    logger.info("Auto-selecting prompt strategy based on patent content...")
                    
                    # ê¸°ë³¸ PDF ë¶„ì„ì„ ìœ„í•´ ì„ì‹œë¡œ base template ì‚¬ìš©
                    temp_template = prompt_manager.get_prompt("base_template")
                    temp_queries = await pipeline.query_generator.generate_queries_from_pdf(
                        args.pdf, temp_template
                    )
                    
                    # ìë™ ì„ íƒ
                    strategy, prompt_template = prompt_manager.auto_select_prompt(
                        temp_queries.get("patent_info", {})
                    )
                    logger.info(f"Auto-selected strategy: {strategy}")
                    
                else:
                    # ì§€ì •ëœ ì „ëµ ì‚¬ìš©
                    strategy = args.strategy
                    prompt_template = prompt_manager.get_prompt(strategy)
                    
            elif args.prompt:
                # ì§ì ‘ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì§€ì •
                if not args.prompt.exists():
                    logger.error(f"Prompt template not found: {args.prompt}")
                    return 1
                prompt_template = load_prompt_template(args.prompt)
                strategy = "custom"
                
            else:
                # ê¸°ë³¸ê°’: base_template ì‚¬ìš©
                strategy = "base_template" 
                prompt_template = prompt_manager.get_prompt(strategy)
                
            logger.info(f"ğŸ¯ Starting single-strategy analysis with: {strategy}")
            # ì „ëµ ì¶”ì ì„ ìœ„í•´ pipeline ê°ì²´ì— ì „ëµ ì„¤ì •
            pipeline._current_strategy = strategy
            results = await pipeline.run_complete_analysis(
                args.pdf, 
                prompt_template, 
                output_dir
            )
            
            # ë‹¨ì¼ ì „ëµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            summary = results.get("summary", {})
            metadata = results.get("metadata", {})
            
            print(f"Pipeline ë¶„ì„ ì™„ë£Œ!")
            print(f"ì „ëµ: {strategy}")
            print(f"íŠ¹í—ˆ: {metadata.get('patent_number', 'Unknown')}")
            print(f"ìƒì„±ëœ ê²€ìƒ‰ì‹: {summary.get('total_queries_generated', 0)}ê°œ")
            print(f"ì„±ê³µí•œ ê²€ìƒ‰: {summary.get('successful_searches', 0)}ê°œ")
            print(f"Seed Recall Rate: {summary.get('seed_recall_rate', 0):.2%}")
            print(f"íƒ€ê²Ÿ ë°œê²¬ ê²€ìƒ‰ì‹: {summary.get('queries_found_target', 0)}ê°œ")
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        pipeline._save_json(results, args.output)
        print(f"ìµœì¢… ê²°ê³¼: {args.output}")
        
        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        if args.multi_strategy:
            analysis_success = results.get("summary", {}).get("analysis_success", False)
        else:
            analysis_success = results.get("summary", {}).get("analysis_success", False)
            
        if not analysis_success:
            logger.warning("âš ï¸  íƒ€ê²Ÿ íŠ¹í—ˆê°€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 2
        
        return 0
        
    except Exception as exc:
        logger.error(f"Pipeline execution failed: {exc}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)