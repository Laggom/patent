"""Seed Recall ê³„ì‚° ë° ê²€ìƒ‰ ì„±ëŠ¥ ë¶„ì„ ë„êµ¬

ì£¼ì–´ì§„ ê²€ìƒ‰ì‹ë“¤ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ì›ë³¸ íŠ¹í—ˆê°€ ê²€ìƒ‰ ê²°ê³¼ì— 
í¬í•¨ë˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ Seed Recallì„ ê³„ì‚°í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
  # JSON ê²€ìƒ‰ì‹ìœ¼ë¡œ Seed Recall ë¶„ì„
  python recall_analyzer.py --queries queries.json --pdf US1234567.pdf --output temp_results/recall.json
  
  # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ (early termination ë¹„í™œì„±í™”)
  python recall_analyzer.py --queries queries.json --pdf patent.pdf --full-recall --output recall.json
  
  # ê²€ìƒ‰ ì§€ì—°ì‹œê°„ ì„¤ì •
  python recall_analyzer.py --queries queries.json --pdf patent.pdf --delay 2.0 --output results.json

ì£¼ìš” ê¸°ëŠ¥:
- ê²€ìƒ‰ì‹ë³„ Google Patents ê²€ìƒ‰ ì‹¤í–‰
- ì›ë³¸ íŠ¹í—ˆ ë°œê²¬ ì—¬ë¶€ í™•ì¸ (Seed Recall)
- ê²€ìƒ‰ ì„±ëŠ¥ í†µê³„ ê³„ì‚°
- Early termination ì§€ì› (ì›ë³¸ íŠ¹í—ˆ ë°œê²¬ì‹œ ì¡°ê¸° ì¢…ë£Œ)
- JSON/CSV í˜•íƒœì˜ ìƒì„¸í•œ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±

í•„ìˆ˜ ì„¤ì •:
1. pip install -r requirements.txt
2. python -m playwright install
"""

import argparse
import asyncio
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from loguru import logger

# patent_downloader ì„í¬íŠ¸
from patent_downloader import GooglePatentsXHRDownloader, PatentSummary


def extract_patent_number_from_filename(pdf_path: Path) -> str:
    """PDF íŒŒì¼ëª…ì—ì„œ íŠ¹í—ˆë²ˆí˜¸ ì¶”ì¶œ (í™•ì¥ì ì œê±°)
    
    ì˜ˆ: US8771637B2.pdf â†’ US8771637B2
    """
    return pdf_path.stem


def normalize_patent_number(patent_number: str) -> str:
    """íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™” (ê³µë°±, ì‰¼í‘œ, í•˜ì´í”ˆ ì œê±° ë° ëŒ€ë¬¸ì ë³€í™˜)
    
    ì˜ˆ: "US 8,771,637 B2" â†’ "US8771637B2"
    """
    if not patent_number:
        return ""
    
    # ê³µë°±, ì‰¼í‘œ, í•˜ì´í”ˆ ì œê±° í›„ ëŒ€ë¬¸ì ë³€í™˜
    normalized = re.sub(r'[\s,\-]', '', patent_number).upper()
    return normalized


class RecallAnalyzer:
    """ê²€ìƒ‰ ì„±ëŠ¥ ë¶„ì„ ë° Seed Recall ê³„ì‚° í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        download_dir: Path,
        max_results: int = 10,
        delay: float = 1.5,
        full_recall: bool = False
    ):
        self.download_dir = download_dir
        self.max_results = max_results
        self.delay = delay
        self.full_recall = full_recall
        
        # Google Patents downloader ì´ˆê¸°í™”
        self.downloader = GooglePatentsXHRDownloader(
            download_dir=download_dir,
            headless=True,
            delay=delay,
            timeout=30
        )
        
    async def execute_searches(
        self, 
        queries_data: Dict[str, Any],
        target_patent: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ì‹ë“¤ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘"""
        logger.info(f"Executing search queries (full_recall={self.full_recall})...")
        
        # target_patent ì„¤ì • (early terminationìš©)
        if target_patent and not self.full_recall:
            self.downloader._target_patent = target_patent
            logger.info(f"Target patent for early termination: {target_patent}")
        
        search_results = []
        queries = queries_data.get("search_queries", [])
        
        for i, query_info in enumerate(queries, 1):
            query = query_info.get("query", "")
            strategy = query_info.get("strategy", f"Query {i}")
            
            logger.info(f"Executing query {i}/{len(queries)}: {strategy}")
            logger.info(f"Query: {query}")
            
            try:
                # ë‹¨ì¼ ì¿¼ë¦¬ ì‹¤í–‰
                saved_files, total_count, patents = await self.downloader.search_and_download(
                    query=query,
                    max_results=self.max_results,
                    count_only=True,  # PDFëŠ” ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šê³  ê²€ìƒ‰ë§Œ
                    full_recall=self.full_recall
                )
                
                # íŠ¹í—ˆ ë²ˆí˜¸ ì¶”ì¶œ
                found_patents = []
                if patents:
                    found_patents = [
                        patent.publication_number 
                        for patent in patents 
                        if patent.publication_number
                    ]
                
                search_results.append({
                    "query_index": i,
                    "strategy": strategy,
                    "query": query,
                    "success": True,
                    "total_results": total_count,
                    "parsed_results": len(patents) if patents else 0,
                    "found_patents": found_patents,
                    "execution_time": 0  # í˜„ì¬ ì¸¡ì •í•˜ì§€ ì•ŠìŒ
                })
                
                logger.info(f"âœ… Query {i} completed: {len(found_patents)} patents found")
                    
            except Exception as exc:
                # ê²€ìƒ‰ ì‹¤íŒ¨
                search_results.append({
                    "query_index": i,
                    "strategy": strategy,
                    "query": query,
                    "success": False,
                    "total_results": None,
                    "parsed_results": 0,
                    "found_patents": [],
                    "error": str(exc)
                })
                
                logger.error(f"âŒ Query {i} failed with error: {exc}")
        
        return search_results
    
    def calculate_seed_recall(
        self, 
        original_patent_number: str,
        search_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Seed Recall ê³„ì‚°"""
        logger.info(f"Calculating Seed Recall for patent: {original_patent_number}")
        
        # íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™”
        normalized_original = normalize_patent_number(original_patent_number)
        
        # ê° ê²€ìƒ‰ì‹ë³„ë¡œ ì›ë³¸ íŠ¹í—ˆ ë°œê²¬ ì—¬ë¶€ í™•ì¸
        query_recalls = []
        found_in_queries = 0
        
        for result in search_results:
            if not result["success"]:
                query_recalls.append({
                    "query_index": result["query_index"],
                    "strategy": result["strategy"], 
                    "query": result["query"],
                    "found": False,
                    "reason": result.get("error", "Query failed")
                })
                continue
            
            found_patents = result.get("found_patents", [])
            normalized_found = [normalize_patent_number(p) for p in found_patents if p]
            
            is_found = normalized_original in normalized_found
            if is_found:
                found_in_queries += 1
                
            query_recalls.append({
                "query_index": result["query_index"],
                "strategy": result["strategy"],
                "query": result["query"],
                "found": is_found,
                "total_results": result["total_results"],
                "parsed_results": result["parsed_results"],
                "found_patents_sample": found_patents[:3] if found_patents else []
            })
            
            logger.info(f"Query {result['query_index']}: {'âœ… Found' if is_found else 'âŒ Not found'} - {result['parsed_results']} results")
        
        # í†µê³„ ê³„ì‚°
        total_queries = len(search_results)
        successful_queries = len([r for r in search_results if r["success"]])
        avg_total_results = sum(
            r["total_results"] or 0 for r in search_results if r["success"]
        ) / max(successful_queries, 1)
        avg_parsed_results = sum(
            r["parsed_results"] for r in search_results
        ) / max(total_queries, 1)
        
        seed_recall_rate = found_in_queries / total_queries if total_queries > 0 else 0
        
        return {
            "original_patent": original_patent_number,
            "normalized_patent": normalized_original,
            "total_queries": total_queries,
            "successful_queries": successful_queries,
            "success_rate": successful_queries / total_queries if total_queries > 0 else 0,
            "avg_total_results": round(avg_total_results, 1),
            "avg_parsed_results": round(avg_parsed_results, 1),
            "seed_recall_rate": seed_recall_rate,
            "queries_found_patent": found_in_queries,
            "query_details": query_recalls
        }
    
    async def analyze_recall(
        self,
        queries_data: Dict[str, Any],
        target_patent_number: str
    ) -> Dict[str, Any]:
        """ì „ì²´ Recall ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        logger.info(f"Starting recall analysis for patent: {target_patent_number}")
        
        # 1. ê²€ìƒ‰ ì‹¤í–‰
        search_results = await self.execute_searches(queries_data, target_patent_number)
        
        # 2. Seed Recall ê³„ì‚°
        performance = self.calculate_seed_recall(target_patent_number, search_results)
        
        # 3. ê²°ê³¼ ì¢…í•©
        final_result = {
            "metadata": {
                "target_patent": target_patent_number,
                "analysis_timestamp": datetime.now().isoformat(),
                "full_recall_mode": self.full_recall,
                "max_results_per_query": self.max_results,
                "search_delay": self.delay
            },
            "search_results": search_results,
            "performance_analysis": performance,
            "queries_metadata": queries_data.get("metadata", {}),
            "patent_info": queries_data.get("patent_info", {})
        }
        
        logger.info(f"Recall analysis completed successfully")
        return final_result


def load_queries_data(queries_path: Path) -> Dict[str, Any]:
    """ê²€ìƒ‰ì‹ ë°ì´í„° ë¡œë“œ"""
    if not queries_path.exists():
        raise FileNotFoundError(f"Queries file not found: {queries_path}")
    
    with open(queries_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_results(results: Dict[str, Any], output_path: Path) -> None:
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Results saved to: {output_path}")


def build_cli_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ êµ¬ì„±"""
    parser = argparse.ArgumentParser(
        description="ê²€ìƒ‰ì‹ ì„±ëŠ¥ ë¶„ì„ ë° Seed Recall ê³„ì‚°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        "--queries", 
        type=Path,
        required=True,
        help="ê²€ìƒ‰ì‹ì´ í¬í•¨ëœ JSON íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--pdf", 
        type=Path,
        required=True,
        help="ëŒ€ìƒ íŠ¹í—ˆ PDF íŒŒì¼ ê²½ë¡œ (íŠ¹í—ˆë²ˆí˜¸ ì¶”ì¶œìš©)"
    )
    
    parser.add_argument(
        "--output", 
        type=Path,
        default=Path("temp_results/recall_analysis.json"),
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: temp_results/recall_analysis.json)"
    )
    
    parser.add_argument(
        "--download-dir",
        type=Path,
        default=Path("./temp_downloads"),
        help="ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: ./temp_downloads)"
    )
    
    parser.add_argument(
        "--max-results",
        type=int,
        default=10,
        help="ê²€ìƒ‰ì‹ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )
    
    parser.add_argument(
        "--delay",
        type=float,
        default=1.5,
        help="ê²€ìƒ‰ ê°„ ì§€ì—°ì‹œê°„(ì´ˆ) (ê¸°ë³¸ê°’: 1.5)"
    )
    
    parser.add_argument(
        "--full-recall",
        action="store_true",
        help="ì „ì²´ ê²€ìƒ‰ ìˆ˜í–‰ (early termination ë¹„í™œì„±í™”)"
    )
    
    return parser


async def main() -> int:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = build_cli_parser()
    args = parser.parse_args()
    
    try:
        # ì…ë ¥ íŒŒì¼ í™•ì¸
        if not args.queries.exists():
            logger.error(f"Queries file not found: {args.queries}")
            return 1
        
        if not args.pdf.exists():
            logger.error(f"PDF file not found: {args.pdf}")
            return 1
        
        # ê²€ìƒ‰ì‹ ë°ì´í„° ë¡œë“œ
        queries_data = load_queries_data(args.queries)
        
        # ëŒ€ìƒ íŠ¹í—ˆë²ˆí˜¸ ì¶”ì¶œ
        target_patent = extract_patent_number_from_filename(args.pdf)
        
        # Recall ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = RecallAnalyzer(
            download_dir=args.download_dir,
            max_results=args.max_results,
            delay=args.delay,
            full_recall=args.full_recall
        )
        
        # Recall ë¶„ì„ ì‹¤í–‰
        results = await analyzer.analyze_recall(queries_data, target_patent)
        
        # ê²°ê³¼ ì €ì¥
        save_results(results, args.output)
        
        # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
        performance = results.get("performance_analysis", {})
        search_results = results.get("search_results", [])
        
        print(f"âœ… Seed Recall ë¶„ì„ ì™„ë£Œ")
        print(f"ğŸ¯ ëŒ€ìƒ íŠ¹í—ˆ: {target_patent}")
        print(f"ğŸ” ì‹¤í–‰ëœ ê²€ìƒ‰ì‹: {len(search_results)}ê°œ")
        print(f"ğŸ“Š Seed Recall Rate: {performance.get('seed_recall_rate', 0):.2%}")
        print(f"âœ… ì„±ê³µí•œ ê²€ìƒ‰: {performance.get('successful_queries', 0)}/{performance.get('total_queries', 0)}")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {args.output}")
        
        # ê° ê²€ìƒ‰ì‹ ê²°ê³¼ ìš”ì•½
        for detail in performance.get("query_details", []):
            status = "âœ…" if detail["found"] else "âŒ"
            print(f"  {detail['query_index']}. {detail['strategy']}: {status}")
        
        return 0
        
    except Exception as exc:
        logger.error(f"Recall analysis failed: {exc}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)